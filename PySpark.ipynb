{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NOfjTmNCuTl"
      },
      "source": [
        "# PySpark Tutorial: Data Processing and Analysis\n",
        "\n",
        "## Overview\n",
        "This notebook covers fundamental PySpark concepts and operations for distributed data processing. PySpark is the Python API for Apache Spark, a powerful distributed computing framework designed for processing large-scale datasets.\n",
        "\n",
        "### Key Topics Covered:\n",
        "1. **SparkSession Initialization** - Entry point for Spark functionality\n",
        "2. **Data Loading** - Reading Parquet files\n",
        "3. **DataFrame Inspection** - Schema, columns, and statistics\n",
        "4. **Data Selection** - Choosing specific columns\n",
        "5. **Data Sorting** - Ordering data by criteria\n",
        "6. **Data Filtering** - Subsetting data based on conditions\n",
        "7. **Data Cleaning** - Handling missing values\n",
        "8. **Feature Engineering** - Creating new columns from existing data\n",
        "9. **Column Renaming** - Restructuring DataFrame columns\n",
        "\n",
        "---\n",
        "[Download dataset](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5_dBir1JrrS",
        "outputId": "96ea66ab-99ff-4c80-dd53-14775914dccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.2)\n",
            "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "NYw0mtutM9jF",
        "outputId": "81563fc9-aa29-451b-a8f2-9d7c44106882"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7da642132ab0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://3019b3030032:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v4.0.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>SparkApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "# Initialize SparkSession - the entry point for Spark functionality\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create or get an existing SparkSession with the name 'SparkApp'\n",
        "spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "IWR04v3lNnvc"
      },
      "outputs": [],
      "source": [
        "# Load data from a Parquet file into a Spark DataFrame\n",
        "# Parquet is a columnar storage format optimized for analytical queries\n",
        "spark_df = spark.read.parquet(\"/content/yellow_tripdata_2025-01.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBwcvb4INnx8",
        "outputId": "22201228-a627-4342-f6bd-1534f1cf4b93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3475226"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "# Count the total number of rows in the DataFrame\n",
        "# Action: This triggers Spark to compute and return the result\n",
        "spark_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N1M2wAZNn0O",
        "outputId": "6509c426-2bab-4232-bf48-29af88df66f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "|       1| 2025-01-01 00:18:38|  2025-01-01 00:26:59|              1|          1.6|         1|                 N|         229|         237|           1|       10.0|  3.5|    0.5|       3.0|         0.0|                  1.0|        18.0|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:32:40|  2025-01-01 00:35:13|              1|          0.5|         1|                 N|         236|         237|           1|        5.1|  3.5|    0.5|      2.02|         0.0|                  1.0|       12.12|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:44:04|  2025-01-01 00:46:01|              1|          0.6|         1|                 N|         141|         141|           1|        5.1|  3.5|    0.5|       2.0|         0.0|                  1.0|        12.1|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:14:27|  2025-01-01 00:20:01|              3|         0.52|         1|                 N|         244|         244|           2|        7.2|  1.0|    0.5|       0.0|         0.0|                  1.0|         9.7|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:21:34|  2025-01-01 00:25:06|              3|         0.66|         1|                 N|         244|         116|           2|        5.8|  1.0|    0.5|       0.0|         0.0|                  1.0|         8.3|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:48:24|  2025-01-01 01:08:26|              2|         2.63|         1|                 N|         239|          68|           2|       19.1|  1.0|    0.5|       0.0|         0.0|                  1.0|        24.1|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:14:47|  2025-01-01 00:16:15|              0|          0.4|         1|                 N|         170|         170|           1|        4.4|  3.5|    0.5|      2.35|         0.0|                  1.0|       11.75|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:39:27|  2025-01-01 00:51:51|              0|          1.6|         1|                 N|         234|         148|           1|       12.1|  3.5|    0.5|       2.0|         0.0|                  1.0|        19.1|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:53:43|  2025-01-01 01:13:23|              0|          2.8|         1|                 N|         148|         170|           1|       19.1|  3.5|    0.5|       3.0|         0.0|                  1.0|        27.1|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:00:02|  2025-01-01 00:09:36|              1|         1.71|         1|                 N|         237|         262|           2|       11.4|  1.0|    0.5|       0.0|         0.0|                  1.0|        16.4|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:20:28|  2025-01-01 00:28:04|              1|         2.29|         1|                 N|         237|          75|           2|       11.4|  1.0|    0.5|       0.0|         0.0|                  1.0|        16.4|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:33:58|  2025-01-01 00:37:23|              1|         0.56|         1|                 N|         263|         236|           1|        5.8|  1.0|    0.5|      2.16|         0.0|                  1.0|       12.96|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:42:40|  2025-01-01 00:55:38|              3|         1.99|         1|                 N|         236|         151|           2|       14.2|  1.0|    0.5|       0.0|         0.0|                  1.0|        19.2|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:30:07|  2025-01-01 00:36:48|              1|          1.1|         1|                 N|         229|         141|           2|        7.9|  3.5|    0.5|       0.0|         0.0|                  1.0|        12.9|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:39:55|  2025-01-01 01:13:59|              1|          3.2|         1|                 N|         141|         113|           1|       26.1|  3.5|    0.5|       7.8|         0.0|                  1.0|        38.9|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:16:54|  2025-01-01 00:35:12|              3|          2.5|         1|                 N|         158|         170|           2|       17.7|  3.5|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:43:10|  2025-01-01 01:00:03|              1|          1.9|         1|                 N|         164|         229|           1|       16.3|  3.5|    0.5|      4.25|         0.0|                  1.0|       25.55|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:01:41|  2025-01-01 00:07:14|              1|         0.71|         1|                 N|          79|         107|           2|       -7.2| -1.0|   -0.5|      3.66|         0.0|                 -1.0|       -8.54|                -2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:01:41|  2025-01-01 00:07:14|              1|         0.71|         1|                 N|          79|         107|           2|        7.2|  1.0|    0.5|       0.0|         0.0|                  1.0|        12.2|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:33:12|  2025-01-01 00:50:14|              2|          1.2|         1|                 N|         246|          90|           1|       15.6|  3.5|    0.5|       0.0|         0.0|                  1.0|        20.6|                 2.5|        0.0|               0.0|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# Display the first 20 rows of the DataFrame\n",
        "# This is useful for a quick visual inspection of the data\n",
        "spark_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44771f22"
      },
      "source": [
        "## 1. SparkSession Initialization\n",
        "\n",
        "**What is SparkSession?**\n",
        "- The entry point for all Spark functionality in PySpark\n",
        "- Represents the connection to a Spark cluster\n",
        "- Enables you to read data, run SQL queries, and perform distributed computations\n",
        "\n",
        "**Key Properties:**\n",
        "- `appName`: Name of your Spark application (useful for tracking in cluster UIs)\n",
        "- Lazily creates a session on first use\n",
        "- Can be reused throughout your script\n",
        "\n",
        "**Why it's important:**\n",
        "All data processing operations in Spark depend on an active SparkSession instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e915d882"
      },
      "source": [
        "## 2. Reading Data with Parquet Files\n",
        "\n",
        "**What is Parquet?**\n",
        "- A columnar storage format developed by Apache\n",
        "- Optimized for analytical queries and data warehousing\n",
        "- Stores data in a compressed format, reducing storage space\n",
        "- Faster than row-based formats like CSV for analytical operations\n",
        "\n",
        "**Why use Parquet?**\n",
        "- Excellent compression (reduces data size by 70-90%)\n",
        "- Efficient column-oriented queries (only reads needed columns)\n",
        "- Preserves data types and schema information\n",
        "- Ideal for big data analytics\n",
        "\n",
        "**The `spark.read.parquet()` method:**\n",
        "- Reads Parquet files into a Spark DataFrame\n",
        "- Automatically infers the schema from the file\n",
        "- Returns a distributed DataFrame object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e13208b"
      },
      "source": [
        "## 3. DataFrame Row Counting\n",
        "\n",
        "**The `count()` Method:**\n",
        "- Returns the total number of rows in the DataFrame\n",
        "- This is an **Action** in Spark - it triggers actual computation\n",
        "- Useful for understanding dataset size and volume\n",
        "\n",
        "**Actions vs Transformations:**\n",
        "- **Actions**: Execute computations and return results (count, show, collect, write)\n",
        "- **Transformations**: Create new DataFrames from existing ones (select, filter, sort) - lazy evaluation\n",
        "\n",
        "**Performance Note:**\n",
        "- For very large datasets, counting can take significant time\n",
        "- Spark must read and process all partitions to get an accurate count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ddf756d"
      },
      "source": [
        "## 4. Viewing DataFrame Data\n",
        "\n",
        "**The `show()` Method:**\n",
        "- Displays the first 20 rows of the DataFrame (by default)\n",
        "- Returns data in a tabular format for easy reading\n",
        "- Useful for quick data inspection and verification\n",
        "- Only evaluates what's needed (truncates long strings)\n",
        "\n",
        "**Parameters:**\n",
        "- `n`: Number of rows to display (default: 20)\n",
        "- `truncate`: Max column width before truncating (default: True)\n",
        "- `vertical`: Display rows vertically instead of horizontally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42307b95"
      },
      "source": [
        "## 5. Inspecting DataFrame Columns\n",
        "\n",
        "**The `columns` Property:**\n",
        "- Returns a Python list of all column names in the DataFrame\n",
        "- Useful for understanding what fields are available\n",
        "- Helps with programmatic column access and validation\n",
        "- Returns column names in the order they appear in the schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5a786bd"
      },
      "source": [
        "## 6. Understanding DataFrame Schema\n",
        "\n",
        "**The `printSchema()` Method:**\n",
        "- Displays the schema in a tree-like format\n",
        "- Shows column names, data types, and nullable properties\n",
        "- Essential for understanding data structure and types\n",
        "- Helps identify potential data type mismatches\n",
        "\n",
        "**What's in the Schema:**\n",
        "- Column name\n",
        "- Data type (String, Integer, Double, Boolean, etc.)\n",
        "- Nullable flag (can column contain NULL values?)\n",
        "- Nested structures for complex data types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d2b3cc6"
      },
      "source": [
        "## 7. Accessing Schema as an Object\n",
        "\n",
        "**The `schema` Property:**\n",
        "- Returns a `StructType` object representing the DataFrame schema\n",
        "- Allows programmatic access to schema details\n",
        "- Useful when you need to manipulate or inspect schema in code\n",
        "- Can be serialized/deserialized for schema management\n",
        "\n",
        "**Schema Inspection Benefits:**\n",
        "- Extract field information programmatically\n",
        "- Validate data types before processing\n",
        "- Generate schema-aware code dynamically"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6a4883f"
      },
      "source": [
        "## 8. Statistical Summary of Data\n",
        "\n",
        "**The `describe()` Method:**\n",
        "- Computes descriptive statistics for numerical columns\n",
        "- Returns count, mean, standard deviation, min, and max values\n",
        "- Called with `.show()` to display results\n",
        "\n",
        "**Statistics Provided:**\n",
        "- **count**: Non-null values in each column\n",
        "- **mean**: Average value\n",
        "- **stddev**: Standard deviation (data spread)\n",
        "- **min**: Minimum value\n",
        "- **max**: Maximum value\n",
        "\n",
        "**Use Cases:**\n",
        "- Quick data quality checks\n",
        "- Identify outliers and data ranges\n",
        "- Understand distribution of numerical data\n",
        "- Detect potential data entry errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SAc0x1VNn2f",
        "outputId": "ec477fc0-76b6-4019-a3a6-20dae405f760"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['VendorID',\n",
              " 'tpep_pickup_datetime',\n",
              " 'tpep_dropoff_datetime',\n",
              " 'passenger_count',\n",
              " 'trip_distance',\n",
              " 'RatecodeID',\n",
              " 'store_and_fwd_flag',\n",
              " 'PULocationID',\n",
              " 'DOLocationID',\n",
              " 'payment_type',\n",
              " 'fare_amount',\n",
              " 'extra',\n",
              " 'mta_tax',\n",
              " 'tip_amount',\n",
              " 'tolls_amount',\n",
              " 'improvement_surcharge',\n",
              " 'total_amount',\n",
              " 'congestion_surcharge',\n",
              " 'Airport_fee',\n",
              " 'cbd_congestion_fee']"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "# Get all column names as a list\n",
        "spark_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXKqkYI1OHQr",
        "outputId": "a1167839-3222-49d9-9227-8af68ffd1d80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- VendorID: integer (nullable = true)\n",
            " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
            " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
            " |-- passenger_count: long (nullable = true)\n",
            " |-- trip_distance: double (nullable = true)\n",
            " |-- RatecodeID: long (nullable = true)\n",
            " |-- store_and_fwd_flag: string (nullable = true)\n",
            " |-- PULocationID: integer (nullable = true)\n",
            " |-- DOLocationID: integer (nullable = true)\n",
            " |-- payment_type: long (nullable = true)\n",
            " |-- fare_amount: double (nullable = true)\n",
            " |-- extra: double (nullable = true)\n",
            " |-- mta_tax: double (nullable = true)\n",
            " |-- tip_amount: double (nullable = true)\n",
            " |-- tolls_amount: double (nullable = true)\n",
            " |-- improvement_surcharge: double (nullable = true)\n",
            " |-- total_amount: double (nullable = true)\n",
            " |-- congestion_surcharge: double (nullable = true)\n",
            " |-- Airport_fee: double (nullable = true)\n",
            " |-- cbd_congestion_fee: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the schema of the DataFrame in a tree format\n",
        "# Shows all column names, data types, and nullable properties\n",
        "spark_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciOC6FTnNn5p",
        "outputId": "3d16b59a-97d8-4f1e-a337-4dbdc637e7c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('VendorID', IntegerType(), True), StructField('tpep_pickup_datetime', TimestampNTZType(), True), StructField('tpep_dropoff_datetime', TimestampNTZType(), True), StructField('passenger_count', LongType(), True), StructField('trip_distance', DoubleType(), True), StructField('RatecodeID', LongType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('payment_type', LongType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('Airport_fee', DoubleType(), True), StructField('cbd_congestion_fee', DoubleType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "# Get the schema as a StructType object for programmatic access\n",
        "spark_df.schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5iM00tdOOYB",
        "outputId": "1ccd5c17-a6a6-47ce-c6bb-cd1127d696fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+---------------------+------------------+--------------------+-------------------+-------------------+\n",
            "|summary|           VendorID|   passenger_count|     trip_distance|        RatecodeID|store_and_fwd_flag|     PULocationID|      DOLocationID|      payment_type|       fare_amount|             extra|            mta_tax|        tip_amount|      tolls_amount|improvement_surcharge|      total_amount|congestion_surcharge|        Airport_fee| cbd_congestion_fee|\n",
            "+-------+-------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+---------------------+------------------+--------------------+-------------------+-------------------+\n",
            "|  count|            3475226|           2935077|           3475226|           2935077|           2935077|          3475226|           3475226|           3475226|           3475226|           3475226|            3475226|           3475226|           3475226|              3475226|           3475226|             2935077|            2935077|            3475226|\n",
            "|   mean| 1.7854283433652949|1.2978589658806226|5.8551261788432925| 2.482534529758504|              NULL|165.1915757421244|164.12517689497028|1.0366229419324096|17.081802760456707| 1.317736691081386| 0.4780990502488183| 2.959812786275976|0.4493081025511922|   0.9547945658785998|25.611291697295062|   2.225237191392253|0.12391105923285829|0.48340928043240927|\n",
            "| stddev|0.42632822130544934| 0.750750275480471|  564.601599634634|11.632772004033159|              NULL|64.52948262355366| 69.40168629828565|0.7013334099485161| 463.4729178173049|1.8615086824178564|0.13746226502954273|3.7796811536124078| 2.002581813990847|   0.2781937753492546| 463.6584784502283|  0.9039932093176659|0.47250898141472714|0.36193065974945887|\n",
            "|    min|                  1|                 0|               0.0|                 1|                 N|                1|                 1|                 0|            -900.0|              -7.5|               -0.5|             -86.0|           -126.94|                 -1.0|            -901.0|                -2.5|              -1.75|              -0.75|\n",
            "|    max|                  7|                 9|         276423.57|                99|                 Y|              265|               265|                 5|         863372.12|              15.0|               10.5|             400.0|            170.94|                  1.0|         863380.37|                 2.5|               6.75|               0.75|\n",
            "+-------+-------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+---------------------+------------------+--------------------+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display descriptive statistics (count, mean, stddev, min, max) for numerical columns\n",
        "spark_df.describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k8uXA7fCuUF"
      },
      "source": [
        "---\n",
        "\n",
        "## Data Transformation Operations\n",
        "\n",
        "### 9. Column Selection (Projection)\n",
        "\n",
        "**The `select()` Method:**\n",
        "- Selects specific columns from a DataFrame\n",
        "- Returns a new DataFrame with only chosen columns\n",
        "- Reduces memory footprint by removing unnecessary data\n",
        "- Can be used with column names or Column objects\n",
        "\n",
        "**Syntax:**\n",
        "- `df.select(['col1', 'col2', ...])` - using list of strings\n",
        "- `df.select('col1', 'col2')` - using variable arguments\n",
        "- `df.select(col('col1'), col('col2'))` - using Column objects\n",
        "\n",
        "**Benefits:**\n",
        "- Improves query performance (only processes needed columns)\n",
        "- Simplifies data by removing irrelevant fields\n",
        "- Foundation for feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoORe1n5OVBz",
        "outputId": "428ae167-446b-4e72-b8a1-679ceff68182"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------+\n",
            "|fare_amount|passenger_count|\n",
            "+-----------+---------------+\n",
            "|       10.0|              1|\n",
            "|        5.1|              1|\n",
            "|        5.1|              1|\n",
            "|        7.2|              3|\n",
            "|        5.8|              3|\n",
            "|       19.1|              2|\n",
            "|        4.4|              0|\n",
            "|       12.1|              0|\n",
            "|       19.1|              0|\n",
            "|       11.4|              1|\n",
            "|       11.4|              1|\n",
            "|        5.8|              1|\n",
            "|       14.2|              3|\n",
            "|        7.9|              1|\n",
            "|       26.1|              1|\n",
            "|       17.7|              3|\n",
            "|       16.3|              1|\n",
            "|       -7.2|              1|\n",
            "|        7.2|              1|\n",
            "|       15.6|              2|\n",
            "+-----------+---------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# Select only specific columns: fare_amount and passenger_count\n",
        "# This creates a new DataFrame with just these two columns\n",
        "spark_df.select(['fare_amount','passenger_count']).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1E1OUl_yPpFY",
        "outputId": "36fef2de-d9b5-4811-e2d9-96244585a0da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "|       2| 2025-01-07 19:12:25|  2025-01-07 19:14:04|              1|          0.1|         5|                 N|         226|         226|           3|     -900.0|  0.0|    0.0|       0.0|         0.0|                 -1.0|      -901.0|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-19 23:52:08|  2025-01-19 23:52:17|              1|          0.0|         5|                 N|         265|         265|           4|     -850.0|  0.0|    0.0|       0.0|         0.0|                 -1.0|      -851.0|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-01 23:13:43|  2025-01-02 01:47:38|              1|       132.27|         4|                 N|         132|         265|           4|     -826.2| -1.0|    0.0|       0.0|      -35.44|                 -1.0|     -865.39|                 0.0|      -1.75|               0.0|\n",
            "|       2| 2025-01-01 13:47:30|  2025-01-01 13:47:40|              1|          0.0|         5|                 N|         265|         265|           4|     -700.0|  0.0|    0.0|       0.0|         0.0|                 -1.0|      -701.0|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-01 13:56:20|  2025-01-01 13:56:24|              1|          0.0|         5|                 N|         265|         265|           4|     -700.0|  0.0|    0.0|       0.0|         0.0|                 -1.0|      -701.0|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-05 21:51:52|  2025-01-05 21:52:14|              1|          0.0|         5|                 N|          74|          74|           2|     -700.0|  0.0|    0.0|       0.0|         0.0|                 -1.0|      -701.0|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-14 02:08:52|  2025-01-14 02:10:20|              1|         0.35|         5|                 N|         230|          48|           4|     -700.0|  0.0|    0.0|       0.0|         0.0|                 -1.0|     -704.25|                -2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-16 08:02:27|  2025-01-16 08:03:02|              4|          0.0|         5|                 N|         132|         132|           4|     -700.0|  0.0|    0.0|      50.0|         0.0|                 -1.0|     -652.75|                 0.0|      -1.75|               0.0|\n",
            "|       2| 2025-01-11 09:50:17|  2025-01-11 12:43:04|              4|        78.94|         4|                 N|         261|         261|           4|     -634.4|  0.0|   -0.5|      20.0|      -14.06|                 -1.0|     -633.21|                -2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-26 11:50:22|  2025-01-26 11:50:28|              3|          0.0|         5|                 N|         138|         138|           2|     -600.0| -5.0|    0.0|       0.0|         0.0|                 -1.0|     -607.75|                 0.0|      -1.75|               0.0|\n",
            "|       2| 2025-01-05 14:24:08|  2025-01-05 14:24:47|              1|          0.0|         5|                 N|         242|         242|           3|     -600.0|  0.0|    0.0|       0.0|         0.0|                 -1.0|      -601.0|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-07 22:50:46|  2025-01-08 00:39:35|              1|        87.92|         4|                 N|         132|         265|           4|     -595.2| -1.0|    0.0|      20.0|       -6.94|                 -1.0|     -585.89|                 0.0|      -1.75|               0.0|\n",
            "|       2| 2025-01-09 12:56:34|  2025-01-09 14:54:51|              2|        81.14|         4|                 N|         132|         265|           3|     -579.8|  0.0|   -0.5|       0.0|      -35.06|                 -1.0|     -616.36|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-12 11:52:37|  2025-01-12 14:23:10|              1|       131.88|         5|                 N|         132|         265|           2|     -550.0|  0.0|    0.0|      15.0|         0.0|                 -1.0|     -537.75|                 0.0|      -1.75|               0.0|\n",
            "|       2| 2025-01-23 15:07:01|  2025-01-23 17:10:43|              1|        86.55|         4|                 N|         132|         265|           2|     -541.3|  0.0|   -0.5|       0.0|       -6.94|                 -1.0|     -551.49|                 0.0|      -1.75|               0.0|\n",
            "|       2| 2025-01-19 17:05:05|  2025-01-19 19:06:22|              1|        86.88|         4|                 N|         132|         265|           4|     -502.8|  0.0|    0.0|      10.0|      -45.94|                 -1.0|     -541.49|                 0.0|      -1.75|               0.0|\n",
            "|       2| 2025-01-27 10:02:37|  2025-01-27 10:02:53|              1|          0.0|         5|                 N|         265|         265|           4|     -500.0|  0.0|    0.0|       0.0|         0.0|                 -1.0|      -501.0|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-14 09:18:16|  2025-01-14 09:18:29|              1|          0.0|         5|                 N|         198|         198|           3|     -500.0|  0.0|    0.0|       0.0|         0.0|                 -1.0|      -501.0|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-28 10:47:58|  2025-01-28 10:48:02|              1|          0.0|         5|                 N|         260|         260|           3|     -499.0|  0.0|    0.0|       0.0|         0.0|                 -1.0|      -500.0|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-28 11:00:35|  2025-01-28 11:00:39|              1|          0.0|         5|                 N|         260|         260|           3|     -499.0|  0.0|    0.0|       0.0|         0.0|                 -1.0|      -500.0|                 0.0|        0.0|               0.0|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# Sort the DataFrame by a single column in ascending order (default)\n",
        "# The sort() method is a transformation that creates a new sorted DataFrame\n",
        "spark_df.sort('fare_amount').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k9y1r9PCuUH"
      },
      "source": [
        "---\n",
        "\n",
        "### 10. Sorting Data\n",
        "\n",
        "**The `sort()` Method:**\n",
        "- Arranges rows based on one or more column values\n",
        "- Returns a new sorted DataFrame\n",
        "- Default order is ascending (smallest to largest)\n",
        "- Can sort by multiple columns with different orders\n",
        "\n",
        "**Parameters:**\n",
        "- Column name or list of column names\n",
        "- `ascending`: Boolean or list of booleans for sort order\n",
        "- Multiple sort criteria applied in order\n",
        "\n",
        "**Use Cases:**\n",
        "- Ranking data by values\n",
        "- Finding top N or bottom N records\n",
        "- Organizing data for presentation\n",
        "- Preparing data for time-series analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e7ft4KzQTnD",
        "outputId": "9b97dc16-10de-4c3b-d041-37431688747f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "|       1| 2025-01-20 12:07:18|  2025-01-20 12:12:42|              1|          1.6|         1|                 N|         138|           8|           4|  863372.12| 6.75|    0.5|       0.0|         0.0|                  1.0|   863380.37|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-16 12:23:14|  2025-01-17 14:41:56|              1|       255.33|         4|                 N|         161|         265|           2|     2450.9|  0.0|    0.0|       0.0|       54.06|                  1.0|     2506.71|                 0.0|        0.0|              0.75|\n",
            "|       2| 2025-01-21 05:17:57|  2025-01-21 08:19:45|              1|       188.88|         4|                 N|         247|         265|           2|     1309.2|  1.0|    0.5|       0.0|         0.0|                  1.0|      1311.7|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-26 00:04:04|  2025-01-26 00:04:42|              4|          0.0|         5|                 N|         142|         142|           2|      950.0|  0.0|    0.0|       0.0|         0.0|                  1.0|       953.5|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-26 00:22:47|  2025-01-26 00:22:55|              4|          0.0|         5|                 N|         226|         226|           2|      950.0|  0.0|    0.0|       0.0|         0.0|                  1.0|       951.0|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-26 00:24:25|  2025-01-26 00:24:35|              4|          0.0|         5|                 N|         226|         226|           2|      950.0|  0.0|    0.0|       0.0|         0.0|                  1.0|       951.0|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-15 19:14:42|  2025-01-15 21:46:13|              1|       143.54|         4|                 N|         132|         265|           1|      936.8|  2.5|    0.0|       0.0|        27.0|                  1.0|      969.05|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-26 00:00:56|  2025-01-26 00:01:06|              4|          0.0|         5|                 N|         142|         142|           2|      900.0|  0.0|    0.0|       0.0|         0.0|                  1.0|       903.5|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-07 19:12:25|  2025-01-07 19:14:04|              1|          0.1|         5|                 N|         226|         226|           3|      900.0|  0.0|    0.0|       0.0|         0.0|                  1.0|       901.0|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-25 23:56:58|  2025-01-25 23:57:06|              4|          0.0|         5|                 N|         142|         142|           2|     899.99|  0.0|    0.0|       0.0|         0.0|                  1.0|      903.49|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-25 23:58:50|  2025-01-25 23:59:00|              4|          0.0|         5|                 N|         142|         142|           2|     899.99|  0.0|    0.0|       0.0|         0.0|                  1.0|      903.49|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-13 17:19:58|  2025-01-13 20:07:08|              1|        133.3|         5|                 N|         132|         265|           1|     893.75| 1.75|    0.0|       0.0|         0.0|                  1.0|       896.5|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-19 23:52:08|  2025-01-19 23:52:17|              1|          0.0|         5|                 N|         265|         265|           4|      850.0|  0.0|    0.0|       0.0|         0.0|                  1.0|       851.0|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-01 23:13:43|  2025-01-02 01:47:38|              1|       132.27|         4|                 N|         132|         265|           4|      826.2|  1.0|    0.0|       0.0|       35.44|                  1.0|      865.39|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-26 19:25:41|  2025-01-26 19:25:56|              1|          0.0|         5|                 N|         216|         216|           1|      800.0|  0.0|    0.0|       0.0|         0.0|                  1.0|       801.0|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-01 12:53:37|  2025-01-01 15:25:57|              1|       119.66|         4|                 N|         101|         265|           2|      773.0|  0.0|    0.5|       0.0|       20.32|                  1.0|      794.82|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-16 08:02:27|  2025-01-16 08:03:02|              4|          0.0|         5|                 N|         132|         132|           4|      700.0|  0.0|    0.0|       0.0|         0.0|                  1.0|      702.75|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 13:47:30|  2025-01-01 13:47:40|              1|          0.0|         5|                 N|         265|         265|           2|      700.0|  0.0|    0.0|       0.0|         0.0|                  1.0|       701.0|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-01 13:56:20|  2025-01-01 13:56:24|              1|          0.0|         5|                 N|         265|         265|           2|      700.0|  0.0|    0.0|       0.0|         0.0|                  1.0|       701.0|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-05 21:51:52|  2025-01-05 21:52:14|              1|          0.0|         5|                 N|          74|          74|           2|      700.0|  0.0|    0.0|       0.0|         0.0|                  1.0|       701.0|                 0.0|        0.0|               0.0|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# Sort by multiple columns in descending order\n",
        "# Sorts first by fare_amount (descending), then by passenger_count (descending)\n",
        "spark_df.sort(['fare_amount',\"passenger_count\"], ascending = [False,False]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-Kmm7azCuUI"
      },
      "source": [
        "---\n",
        "\n",
        "### 11. Filtering Data (Selection)\n",
        "\n",
        "**The `filter()` Method:**\n",
        "- Selects rows that satisfy a condition\n",
        "- Returns a new DataFrame with only matching rows\n",
        "- Critical for data cleaning and subsetting\n",
        "- Conditions can use SQL syntax or PySpark expressions\n",
        "\n",
        "**Filter Syntax Options:**\n",
        "1. **SQL String**: `df.filter('column > 100')`\n",
        "2. **PySpark Expression**: `df.filter(df['column'] > 100)`\n",
        "3. **Column Object**: `df.filter(col('column') > 100)`\n",
        "\n",
        "**Combining Conditions:**\n",
        "- AND operator: `&` (not `and`)\n",
        "- OR operator: `|` (not `or`)\n",
        "- NOT operator: `~` (not `not`)\n",
        "- Must wrap conditions in parentheses when combining\n",
        "\n",
        "**Performance Considerations:**\n",
        "- Filter early to reduce data size\n",
        "- More selective filters improve performance\n",
        "- Spark optimizes filter operations automatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga5EAodHQzxQ",
        "outputId": "08792d61-6fa8-4718-bfda-e26016c597b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "|       1| 2025-01-01 00:51:41|  2025-01-01 01:06:26|              1|          7.2|         1|                 N|         132|          95|           1|       29.6| 2.75|    0.5|      6.75|         0.0|                  1.0|        40.6|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:55:44|  2025-01-01 01:27:41|              1|        14.84|         1|                 N|         132|          89|           1|       59.0|  1.0|    0.5|      12.3|         0.0|                  1.0|       75.55|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:02:20|  2025-01-01 00:15:55|              1|         4.55|         1|                 N|         138|         146|           1|       19.8|  6.0|    0.5|      8.19|         0.0|                  1.0|       37.24|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:08:07|  2025-01-01 00:13:06|              1|         1.61|         1|                 N|         138|         138|           2|        9.3|  6.0|    0.5|       0.0|         0.0|                  1.0|       18.55|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:24:51|  2025-01-01 00:42:52|              2|         8.66|         1|                 N|         138|         256|           1|       35.2|  6.0|    0.5|      4.44|         0.0|                  1.0|       48.89|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:11:59|  2025-01-01 00:48:37|              2|         9.17|         1|                 N|         138|         186|           1|       48.5|  6.0|    0.5|     13.09|        6.94|                  1.0|       80.28|                 2.5|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:39:59|  2025-01-01 01:00:37|              1|         7.78|         1|                 N|         138|          36|           1|       33.8|  6.0|    0.5|      8.26|         0.0|                  1.0|       51.31|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:32:51|  2025-01-01 00:54:01|              1|         9.58|         1|                 N|         132|          77|           2|       39.4|  1.0|    0.5|       0.0|         0.0|                  1.0|       43.65|                 0.0|       1.75|               0.0|\n",
            "|       1| 2025-01-01 00:03:00|  2025-01-01 00:25:10|              2|         10.4|         1|                 N|         138|         162|           1|       40.8|10.25|    0.5|       0.0|        6.94|                  1.0|       59.49|                 2.5|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:09:22|  2025-01-01 00:20:00|              1|         6.09|         1|                 N|         138|         112|           2|       24.7|  6.0|    0.5|       0.0|         0.0|                  1.0|       33.95|                 0.0|       1.75|               0.0|\n",
            "|       1| 2025-01-01 00:23:05|  2025-01-01 00:56:38|              1|         22.0|         1|                 N|         132|         241|           2|       80.7| 2.75|    0.5|       0.0|        6.94|                  1.0|       91.89|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:17:04|  2025-01-01 00:39:02|              1|         9.82|         1|                 N|         132|         122|           2|       40.1|  1.0|    0.5|       0.0|         0.0|                  1.0|       44.35|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:01:41|  2025-01-01 00:15:29|              1|         7.15|         1|                 N|         138|          80|           1|       29.6|  6.0|    0.5|      40.0|         0.0|                  1.0|       78.85|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:36:08|  2025-01-01 00:54:52|              4|        11.86|         1|                 N|         138|         241|           1|       45.0|  6.0|    0.5|      30.0|        6.94|                  1.0|       91.19|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:03:30|  2025-01-01 00:16:37|              3|         5.81|         1|                 N|          70|          95|           1|       24.7|  6.0|    0.5|      6.44|         0.0|                  1.0|       40.39|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:39:58|  2025-01-01 01:01:26|              1|         8.02|         1|                 N|         138|         112|           1|       35.2|  6.0|    0.5|      8.54|         0.0|                  1.0|       52.99|                 0.0|       1.75|               0.0|\n",
            "|       1| 2025-01-01 00:24:10|  2025-01-01 00:32:36|              1|          3.2|         1|                 N|         138|           7|           1|       14.9| 7.75|    0.5|       6.0|         0.0|                  1.0|       30.15|                 0.0|       1.75|               0.0|\n",
            "|       1| 2025-01-01 00:47:18|  2025-01-01 01:00:47|              2|          8.9|         1|                 N|         138|         140|           1|       34.5|10.25|    0.5|      5.32|        6.94|                  1.0|       58.51|                 2.5|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:18:04|  2025-01-01 00:30:56|              1|         7.41|         1|                 N|         138|         217|           1|       30.3|  6.0|    0.5|      7.56|         0.0|                  1.0|       47.11|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:13:00|  2025-01-01 00:30:33|              1|         7.64|         1|                 N|         138|          36|           1|       32.4|  6.0|    0.5|       4.0|         0.0|                  1.0|       45.65|                 0.0|       1.75|               0.0|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# Filter for rows where Airport_fee is greater than 0\n",
        "# Uses SQL string syntax for the filter condition\n",
        "spark_df.filter('Airport_fee >0').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHxleIR8Repi",
        "outputId": "9dff9e0a-83c6-410b-b08d-1ed53bdba9a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "|       1| 2025-01-01 00:18:38|  2025-01-01 00:26:59|              1|          1.6|         1|                 N|         229|         237|           1|       10.0|  3.5|    0.5|       3.0|         0.0|                  1.0|        18.0|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:32:40|  2025-01-01 00:35:13|              1|          0.5|         1|                 N|         236|         237|           1|        5.1|  3.5|    0.5|      2.02|         0.0|                  1.0|       12.12|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:44:04|  2025-01-01 00:46:01|              1|          0.6|         1|                 N|         141|         141|           1|        5.1|  3.5|    0.5|       2.0|         0.0|                  1.0|        12.1|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:14:27|  2025-01-01 00:20:01|              3|         0.52|         1|                 N|         244|         244|           2|        7.2|  1.0|    0.5|       0.0|         0.0|                  1.0|         9.7|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:21:34|  2025-01-01 00:25:06|              3|         0.66|         1|                 N|         244|         116|           2|        5.8|  1.0|    0.5|       0.0|         0.0|                  1.0|         8.3|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:48:24|  2025-01-01 01:08:26|              2|         2.63|         1|                 N|         239|          68|           2|       19.1|  1.0|    0.5|       0.0|         0.0|                  1.0|        24.1|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:14:47|  2025-01-01 00:16:15|              0|          0.4|         1|                 N|         170|         170|           1|        4.4|  3.5|    0.5|      2.35|         0.0|                  1.0|       11.75|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:39:27|  2025-01-01 00:51:51|              0|          1.6|         1|                 N|         234|         148|           1|       12.1|  3.5|    0.5|       2.0|         0.0|                  1.0|        19.1|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:53:43|  2025-01-01 01:13:23|              0|          2.8|         1|                 N|         148|         170|           1|       19.1|  3.5|    0.5|       3.0|         0.0|                  1.0|        27.1|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:20:28|  2025-01-01 00:28:04|              1|         2.29|         1|                 N|         237|          75|           2|       11.4|  1.0|    0.5|       0.0|         0.0|                  1.0|        16.4|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:33:58|  2025-01-01 00:37:23|              1|         0.56|         1|                 N|         263|         236|           1|        5.8|  1.0|    0.5|      2.16|         0.0|                  1.0|       12.96|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:42:40|  2025-01-01 00:55:38|              3|         1.99|         1|                 N|         236|         151|           2|       14.2|  1.0|    0.5|       0.0|         0.0|                  1.0|        19.2|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:30:07|  2025-01-01 00:36:48|              1|          1.1|         1|                 N|         229|         141|           2|        7.9|  3.5|    0.5|       0.0|         0.0|                  1.0|        12.9|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:39:55|  2025-01-01 01:13:59|              1|          3.2|         1|                 N|         141|         113|           1|       26.1|  3.5|    0.5|       7.8|         0.0|                  1.0|        38.9|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:16:54|  2025-01-01 00:35:12|              3|          2.5|         1|                 N|         158|         170|           2|       17.7|  3.5|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:43:10|  2025-01-01 01:00:03|              1|          1.9|         1|                 N|         164|         229|           1|       16.3|  3.5|    0.5|      4.25|         0.0|                  1.0|       25.55|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:33:12|  2025-01-01 00:50:14|              2|          1.2|         1|                 N|         246|          90|           1|       15.6|  3.5|    0.5|       0.0|         0.0|                  1.0|        20.6|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:34:40|  2025-01-01 00:51:19|              2|         1.19|         1|                 N|         246|         170|           1|       14.9|  1.0|    0.5|      3.98|         0.0|                  1.0|       23.88|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:55:54|  2025-01-01 01:00:38|              1|         0.69|         1|                 N|         137|         233|           4|       -6.5| -1.0|   -0.5|       0.0|         0.0|                 -1.0|       -11.5|                -2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:55:54|  2025-01-01 01:00:38|              1|         0.69|         1|                 N|         137|         233|           4|        6.5|  1.0|    0.5|       0.0|         0.0|                  1.0|        11.5|                 2.5|        0.0|               0.0|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# Filter for rides where pickup time is after a specific datetime\n",
        "# Uses PySpark expression syntax with column reference\n",
        "spark_df.filter(spark_df['tpep_pickup_datetime']>'2025-01-01 00:11:59').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoSIytQyRqhI",
        "outputId": "0df09d73-3c55-488d-c530-3aea1cd2bacc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "|       1| 2025-01-01 00:51:41|  2025-01-01 01:06:26|              1|          7.2|         1|                 N|         132|          95|           1|       29.6| 2.75|    0.5|      6.75|         0.0|                  1.0|        40.6|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:55:44|  2025-01-01 01:27:41|              1|        14.84|         1|                 N|         132|          89|           1|       59.0|  1.0|    0.5|      12.3|         0.0|                  1.0|       75.55|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:24:51|  2025-01-01 00:42:52|              2|         8.66|         1|                 N|         138|         256|           1|       35.2|  6.0|    0.5|      4.44|         0.0|                  1.0|       48.89|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:39:59|  2025-01-01 01:00:37|              1|         7.78|         1|                 N|         138|          36|           1|       33.8|  6.0|    0.5|      8.26|         0.0|                  1.0|       51.31|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:32:51|  2025-01-01 00:54:01|              1|         9.58|         1|                 N|         132|          77|           2|       39.4|  1.0|    0.5|       0.0|         0.0|                  1.0|       43.65|                 0.0|       1.75|               0.0|\n",
            "|       1| 2025-01-01 00:23:05|  2025-01-01 00:56:38|              1|         22.0|         1|                 N|         132|         241|           2|       80.7| 2.75|    0.5|       0.0|        6.94|                  1.0|       91.89|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:17:04|  2025-01-01 00:39:02|              1|         9.82|         1|                 N|         132|         122|           2|       40.1|  1.0|    0.5|       0.0|         0.0|                  1.0|       44.35|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:36:08|  2025-01-01 00:54:52|              4|        11.86|         1|                 N|         138|         241|           1|       45.0|  6.0|    0.5|      30.0|        6.94|                  1.0|       91.19|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:39:58|  2025-01-01 01:01:26|              1|         8.02|         1|                 N|         138|         112|           1|       35.2|  6.0|    0.5|      8.54|         0.0|                  1.0|       52.99|                 0.0|       1.75|               0.0|\n",
            "|       1| 2025-01-01 00:24:10|  2025-01-01 00:32:36|              1|          3.2|         1|                 N|         138|           7|           1|       14.9| 7.75|    0.5|       6.0|         0.0|                  1.0|       30.15|                 0.0|       1.75|               0.0|\n",
            "|       1| 2025-01-01 00:47:18|  2025-01-01 01:00:47|              2|          8.9|         1|                 N|         138|         140|           1|       34.5|10.25|    0.5|      5.32|        6.94|                  1.0|       58.51|                 2.5|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:18:04|  2025-01-01 00:30:56|              1|         7.41|         1|                 N|         138|         217|           1|       30.3|  6.0|    0.5|      7.56|         0.0|                  1.0|       47.11|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:13:00|  2025-01-01 00:30:33|              1|         7.64|         1|                 N|         138|          36|           1|       32.4|  6.0|    0.5|       4.0|         0.0|                  1.0|       45.65|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:16:58|  2025-01-01 00:28:36|              1|         6.59|         1|                 N|         138|          95|           1|       26.1|  6.0|    0.5|     10.08|         0.0|                  1.0|       45.43|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:49:26|  2025-01-01 01:14:47|              1|        12.15|         1|                 N|         138|          79|           1|       49.2|  6.0|    0.5|       0.0|        6.94|                  1.0|       67.89|                 2.5|       1.75|               0.0|\n",
            "|       1| 2025-01-01 00:28:24|  2025-01-01 00:52:22|              1|         12.2|         1|                 N|         138|          35|           1|       48.5| 7.75|    0.5|     11.55|         0.0|                  1.0|        69.3|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:57:11|  2025-01-01 01:22:09|              2|         9.74|         1|                 N|         138|         235|           2|       42.9|  6.0|    0.5|       0.0|        6.94|                  1.0|       59.09|                 0.0|       1.75|               0.0|\n",
            "|       1| 2025-01-01 00:49:54|  2025-01-01 01:05:50|              1|          7.7|         1|                 N|         138|         263|           1|       30.3|10.25|    0.5|       9.8|        6.94|                  1.0|       58.79|                 2.5|       1.75|               0.0|\n",
            "|       1| 2025-01-01 00:21:10|  2025-01-01 00:37:59|              1|          9.6|         1|                 N|         138|         229|           1|       37.3|10.25|    0.5|      10.0|        6.94|                  1.0|       65.99|                 2.5|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:54:22|  2025-01-01 01:14:51|              4|          9.3|         4|                 N|         132|         265|           1|       47.1|  1.0|    0.5|      9.92|         0.0|                  1.0|       61.27|                 0.0|       1.75|               0.0|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# Filter for rows that meet BOTH conditions using AND operator (&)\n",
        "# Airport_fee must be greater than 0 AND pickup time must be after specified time\n",
        "# Note: Must use & (bitwise AND) not 'and', and wrap conditions in parentheses\n",
        "spark_df.filter((spark_df['Airport_fee']>0) & (spark_df['tpep_pickup_datetime']> '2025-01-01 00:11:59')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPC3QF_4SBEh",
        "outputId": "323bc16e-6013-49a9-e972-55a843c84d29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------------+------------+\n",
            "|VendorId|passenger_count|total_amount|\n",
            "+--------+---------------+------------+\n",
            "|       2|              3|         9.7|\n",
            "|       2|              3|         8.3|\n",
            "|       2|              3|        19.2|\n",
            "|       1|              3|        22.7|\n",
            "|       1|              3|       15.45|\n",
            "|       2|              3|       30.13|\n",
            "|       2|              4|       50.76|\n",
            "|       2|              4|       29.76|\n",
            "|       2|              4|       16.13|\n",
            "|       1|              4|       37.95|\n",
            "|       2|              9|      111.32|\n",
            "|       2|              4|        23.4|\n",
            "|       1|              4|        12.2|\n",
            "|       2|              4|       36.48|\n",
            "|       1|              3|       12.29|\n",
            "|       2|              4|       28.08|\n",
            "|       1|              4|        16.4|\n",
            "|       2|              3|       56.38|\n",
            "|       2|              3|        34.8|\n",
            "|       1|              4|       17.15|\n",
            "+--------+---------------+------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# Combine multiple operations: filter, select, and display\n",
        "# Step 1: Filter for rides with more than 2 passengers\n",
        "# Step 2: Select only VendorId, passenger_count, and total_amount columns\n",
        "# Step 3: Display the results\n",
        "spark_df.filter(spark_df['passenger_count']>2)\\\n",
        ".select(['VendorId','passenger_count','total_amount'])\\\n",
        ".show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsRwEFbdSBSd",
        "outputId": "385dda89-b032-4e60-bd53-92100bc380ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------+\n",
            "|passenger_count|Airport_fee|\n",
            "+---------------+-----------+\n",
            "|              1|        0.0|\n",
            "|              1|        0.0|\n",
            "|              1|        0.0|\n",
            "|              1|        0.0|\n",
            "|              1|        0.0|\n",
            "|              1|        0.0|\n",
            "|              1|        0.0|\n",
            "|              1|        0.0|\n",
            "|              1|        0.0|\n",
            "|              1|        0.0|\n",
            "|              1|        0.0|\n",
            "|              1|        0.0|\n",
            "|              1|        0.0|\n",
            "|              1|        0.0|\n",
            "|              1|        0.0|\n",
            "|              1|        0.0|\n",
            "|              1|        0.0|\n",
            "|              1|        0.0|\n",
            "|              1|        0.0|\n",
            "|              1|        0.0|\n",
            "+---------------+-----------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# Challenge: Write a query combining sort, select, and filter\n",
        "# Find non-airport rides with exactly 1 passenger, sort and show relevant columns\n",
        "# Step 1: Filter for Airport_fee == 0 (non-airport) AND passenger_count == 1\n",
        "# Step 2: Sort by Airport_fee\n",
        "# Step 3: Select only passenger_count and Airport_fee columns\n",
        "spark_df.filter((spark_df['Airport_fee']==0) & (spark_df['passenger_count']==1))\\\n",
        ".sort('Airport_fee')\\\n",
        ".select('passenger_count','Airport_fee')\\\n",
        ".show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qRhEAjZSBY5",
        "outputId": "33d5369b-8c3f-40d9-f4b8-7e3c17b73da6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------------+\n",
            "|trip_distance|total_amount|\n",
            "+-------------+------------+\n",
            "|          1.6|        18.0|\n",
            "|          0.5|       12.12|\n",
            "|          0.6|        12.1|\n",
            "|         0.52|         9.7|\n",
            "|         0.66|         8.3|\n",
            "|         2.63|        24.1|\n",
            "|          0.4|       11.75|\n",
            "|          1.6|        19.1|\n",
            "|          2.8|        27.1|\n",
            "|         1.71|        16.4|\n",
            "|         2.29|        16.4|\n",
            "|         0.56|       12.96|\n",
            "|         1.99|        19.2|\n",
            "|          1.1|        12.9|\n",
            "|          3.2|        38.9|\n",
            "|          2.5|        22.7|\n",
            "|          1.9|       25.55|\n",
            "|         0.71|       -8.54|\n",
            "|         0.71|        12.2|\n",
            "|          1.2|        20.6|\n",
            "+-------------+------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# Challenge: Select only trip distance and total_amount columns\n",
        "spark_df.select(['trip_distance', 'total_amount']).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOf49DGNSBdx",
        "outputId": "65447ce8-f04a-4eae-ab58-c9c540be8d7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------------+\n",
            "|trip_distance|total_amount|\n",
            "+-------------+------------+\n",
            "|    276423.57|         5.0|\n",
            "|    276099.95|       13.88|\n",
            "|    222167.49|       35.94|\n",
            "|    206137.99|       29.64|\n",
            "|    202771.63|       14.85|\n",
            "|    189687.43|       17.45|\n",
            "|    181139.99|       11.08|\n",
            "|    168079.57|        4.49|\n",
            "|    167452.94|        5.45|\n",
            "|    164959.95|       18.05|\n",
            "|    158925.09|       14.82|\n",
            "|    156037.94|       33.49|\n",
            "|    143712.27|       12.51|\n",
            "|    135116.83|       36.52|\n",
            "|    134033.15|       22.89|\n",
            "|    124083.23|        15.0|\n",
            "|    121799.97|       31.88|\n",
            "|    121555.16|        8.46|\n",
            "|    118927.12|        3.68|\n",
            "|    118435.89|       19.59|\n",
            "+-------------+------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# Challenge: Sort the resulting dataframe by trip distance in descending order\n",
        "# Shows longest trips first\n",
        "spark_df.select(['trip_distance', 'total_amount'])\\\n",
        ".sort(\"trip_distance\", ascending=False).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU9zYsUZSBja",
        "outputId": "0552d78f-b8c8-46c9-ea03-9d1182531be8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------------+\n",
            "|trip_distance|total_amount|\n",
            "+-------------+------------+\n",
            "|      44730.3|        45.0|\n",
            "|      44684.1|       54.94|\n",
            "|      33588.9|        30.0|\n",
            "|      11187.2|       61.94|\n",
            "|      2001.95|       19.35|\n",
            "|      1847.61|       43.37|\n",
            "|      1472.37|       20.58|\n",
            "|        265.9|      139.14|\n",
            "|       255.33|     2506.71|\n",
            "|       206.45|      243.32|\n",
            "|        199.3|        19.0|\n",
            "|       188.88|      1311.7|\n",
            "|        181.9|       39.94|\n",
            "|       150.11|      501.75|\n",
            "|        148.3|      549.91|\n",
            "|       122.77|        64.7|\n",
            "|       119.66|      794.82|\n",
            "|       114.25|       396.0|\n",
            "|       105.24|       361.2|\n",
            "|       104.21|      249.53|\n",
            "+-------------+------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# Combined challenge problem: Filter, Select, Sort\n",
        "# Find all non-airport solo rides and show them sorted by distance (longest first)\n",
        "spark_df.filter((spark_df['Airport_fee']==0) & (spark_df['passenger_count']==1))\\\n",
        ".select('trip_distance', 'total_amount')\\\n",
        ".sort('trip_distance', ascending=False)\\\n",
        ".show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "zWRHRGAXYxEz"
      },
      "outputs": [],
      "source": [
        "# Import necessary functions for missing value detection\n",
        "from pyspark.sql.functions import col, isnull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiZiksv0CuUN"
      },
      "source": [
        "---\n",
        "\n",
        "## Data Cleaning and Missing Values\n",
        "\n",
        "### 12. Detecting and Handling Missing Values\n",
        "\n",
        "**Missing Data in DataFrames:**\n",
        "- Missing values are represented as `NULL` in Spark\n",
        "- Can skew analysis and cause errors in computations\n",
        "- Must be identified and handled appropriately\n",
        "\n",
        "**The `isnull()` Function:**\n",
        "- Returns True where values are NULL\n",
        "- Often combined with `filter()` to count missing values\n",
        "- Can be used to create masks for data cleaning\n",
        "\n",
        "**Common Missing Value Strategies:**\n",
        "1. **Removal**: Delete rows with NULL values (loses data)\n",
        "2. **Imputation**: Fill with default, mean, median, or forward-filled values\n",
        "3. **Domain-specific**: Use business rules to fill values\n",
        "4. **Keep separate**: Mark and analyze NULL values separately\n",
        "\n",
        "**The `fillna()` Method:**\n",
        "- Replaces NULL values with specified values\n",
        "- Can use a dictionary to fill different columns differently\n",
        "- Returns a new DataFrame with filled values\n",
        "- Useful for imputation strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H77GQJJPaLgd",
        "outputId": "90768f7f-728f-45d2-82d1-93ed1dd19428"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "# Count the number of NULL/missing values in the 'fare_amount' column\n",
        "# isnull() returns True for NULL values, count() counts them\n",
        "spark_df.filter(isnull(col('fare_amount'))).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvBD9aeCaUsN",
        "outputId": "722cfcdc-4b28-4fb8-dc5d-6e934e942855"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "540149"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "# Count the number of NULL/missing values in the 'passenger_count' column\n",
        "spark_df.filter(isnull(col('passenger_count'))).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "yq9fakAUaqk5"
      },
      "outputs": [],
      "source": [
        "# Fill missing values in 'passenger_count' column with default value of 1\n",
        "# This assumes if passenger count is missing, there was 1 passenger\n",
        "# Returns a new DataFrame with filled values\n",
        "df = spark_df.fillna({'passenger_count':1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBwZfPYubNnc",
        "outputId": "9b033bad-569b-48af-fcfb-0bfaff490cbb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "# Verify that missing values in 'passenger_count' have been filled\n",
        "# After fillna(), the count should be 0\n",
        "df.filter(isnull(col('passenger_count'))).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "PhSQmuFXd4V2"
      },
      "outputs": [],
      "source": [
        "# Import functions needed for feature engineering\n",
        "# unix_timestamp(): Convert datetime to seconds since epoch\n",
        "# round(): Round numerical values to specified decimal places\n",
        "from pyspark.sql.functions import unix_timestamp, round"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_AiQcSVCuUp"
      },
      "source": [
        "---\n",
        "\n",
        "### 13. Feature Engineering - Creating New Columns\n",
        "\n",
        "**What is Feature Engineering?**\n",
        "- Process of creating new features (columns) from existing data\n",
        "- Transforms raw data into meaningful features for analysis\n",
        "- Critical step in data preprocessing and machine learning\n",
        "\n",
        "**The `withColumn()` Method:**\n",
        "- Adds a new column to a DataFrame or modifies existing ones\n",
        "- Returns a new DataFrame with the added/modified column\n",
        "- Accepts column name and expression\n",
        "- Can chain multiple `withColumn()` calls\n",
        "\n",
        "**Common PySpark Functions:**\n",
        "- `unix_timestamp()`: Converts datetime to Unix timestamp (seconds since 1970)\n",
        "- `round()`: Rounds numerical values to specified decimal places\n",
        "- String functions: concat, substring, length, upper, lower\n",
        "- Math functions: abs, sqrt, pow, etc.\n",
        "- Date functions: year, month, day, hour, minute\n",
        "\n",
        "**Example: Trip Duration Calculation**\n",
        "- Extract pickup and dropoff times\n",
        "- Convert to Unix timestamps\n",
        "- Calculate difference in seconds\n",
        "- Convert to minutes and round"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TU8X71qdbJek",
        "outputId": "705464fb-4d6c-4686-8725-dea2017f6935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|trip_duration|\n",
            "+-------------+\n",
            "|          8.4|\n",
            "|          2.6|\n",
            "|          2.0|\n",
            "|          5.6|\n",
            "|          3.5|\n",
            "|         20.0|\n",
            "|          1.5|\n",
            "|         12.4|\n",
            "|         19.7|\n",
            "|          9.6|\n",
            "|          7.6|\n",
            "|          3.4|\n",
            "|         13.0|\n",
            "|          6.7|\n",
            "|         34.1|\n",
            "|         18.3|\n",
            "|         16.9|\n",
            "|          5.6|\n",
            "|          5.6|\n",
            "|         17.0|\n",
            "+-------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# Feature Engineering: Create a new column 'trip_duration' in minutes\n",
        "# Step 1: Convert dropoff time to Unix timestamp (seconds)\n",
        "# Step 2: Convert pickup time to Unix timestamp (seconds)\n",
        "# Step 3: Calculate difference in seconds\n",
        "# Step 4: Divide by 60 to convert to minutes\n",
        "# Step 5: Round to 1 decimal place\n",
        "# withColumn() creates a new DataFrame with the added column\n",
        "df1 = df.withColumn('trip_duration', \\\n",
        "      round((unix_timestamp('tpep_dropoff_datetime') - unix_timestamp('tpep_pickup_datetime')) / 60, 1))\n",
        "df1.select('trip_duration').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85f375aa",
        "outputId": "3eaccb71-80d8-4499-a1c7-1333fd24353a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "117"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "# Data Quality Check: Count negative trip durations\n",
        "# Negative values indicate data issues (dropoff before pickup)\n",
        "# This helps identify potential data quality problems\n",
        "df1.filter(df1['trip_duration'] < 0).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1dea6af"
      },
      "source": [
        "### Data Quality Analysis - Negative Trip Duration Check\n",
        "\n",
        "**What does this check tell us?**\n",
        "\n",
        "If the count of negative `trip_duration` values is **zero**:\n",
        "-  All trip durations are positive or zero\n",
        "-  Indicates the calculation is accurate in terms of time order\n",
        "-  No data quality issues detected\n",
        "-  Pickup times are correctly recorded before dropoff times\n",
        "\n",
        "If there are **negative values**:\n",
        "-  Implies data quality issues\n",
        "-  Drop-off time recorded before pick-up time (impossible for valid trips)\n",
        "-  Might indicate:\n",
        "  - Incorrect timestamp recording\n",
        "  - System clock issues during data capture\n",
        "  - Data entry errors\n",
        "  - Records that need investigation and possible removal\n",
        "\n",
        "**How to Handle Negative Durations:**\n",
        "1. Filter them out for analysis (treat as invalid records)\n",
        "2. Investigate the source data for systematic issues\n",
        "3. Apply business logic (e.g., if duration < some threshold, mark as suspicious)\n",
        "4. Consider removing outliers and anomalies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLnbnhtyfnN9",
        "outputId": "c78f16a9-6a13-4a4a-d1ac-0f8e1bfee6f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------------------+-----------+\n",
            "|        pu_datetime|        do_datetime|ride-amount|\n",
            "+-------------------+-------------------+-----------+\n",
            "|2025-01-01 00:18:38|2025-01-01 00:26:59|       10.0|\n",
            "|2025-01-01 00:32:40|2025-01-01 00:35:13|        5.1|\n",
            "|2025-01-01 00:44:04|2025-01-01 00:46:01|        5.1|\n",
            "|2025-01-01 00:14:27|2025-01-01 00:20:01|        7.2|\n",
            "|2025-01-01 00:21:34|2025-01-01 00:25:06|        5.8|\n",
            "|2025-01-01 00:48:24|2025-01-01 01:08:26|       19.1|\n",
            "|2025-01-01 00:14:47|2025-01-01 00:16:15|        4.4|\n",
            "|2025-01-01 00:39:27|2025-01-01 00:51:51|       12.1|\n",
            "|2025-01-01 00:53:43|2025-01-01 01:13:23|       19.1|\n",
            "|2025-01-01 00:00:02|2025-01-01 00:09:36|       11.4|\n",
            "|2025-01-01 00:20:28|2025-01-01 00:28:04|       11.4|\n",
            "|2025-01-01 00:33:58|2025-01-01 00:37:23|        5.8|\n",
            "|2025-01-01 00:42:40|2025-01-01 00:55:38|       14.2|\n",
            "|2025-01-01 00:30:07|2025-01-01 00:36:48|        7.9|\n",
            "|2025-01-01 00:39:55|2025-01-01 01:13:59|       26.1|\n",
            "|2025-01-01 00:16:54|2025-01-01 00:35:12|       17.7|\n",
            "|2025-01-01 00:43:10|2025-01-01 01:00:03|       16.3|\n",
            "|2025-01-01 00:01:41|2025-01-01 00:07:14|       -7.2|\n",
            "|2025-01-01 00:01:41|2025-01-01 00:07:14|        7.2|\n",
            "|2025-01-01 00:33:12|2025-01-01 00:50:14|       15.6|\n",
            "+-------------------+-------------------+-----------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# Rename columns for clarity and consistency\n",
        "# Step 1: Select specific columns\n",
        "# Step 2: Rename them using withColumnsRenamed()\n",
        "#   - 'tpep_pickup_datetime'  'pu_datetime' (shorter, clearer)\n",
        "#   - 'tpep_dropoff_datetime'  'do_datetime' (shorter, clearer)\n",
        "#   - 'fare_amount'  'ride-amount' (alternative naming)\n",
        "# This creates a new DataFrame with renamed columns\n",
        "df2 = df1.select('tpep_pickup_datetime','tpep_dropoff_datetime','fare_amount')\\\n",
        ".withColumnsRenamed({'tpep_pickup_datetime':'pu_datetime','tpep_dropoff_datetime':'do_datetime', 'fare_amount':'ride-amount'})\n",
        "\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAAfmL1_CuUs"
      },
      "source": [
        "---\n",
        "\n",
        "### 14. Renaming Columns\n",
        "\n",
        "**The `withColumnsRenamed()` Method:**\n",
        "- Renames one or more columns in a DataFrame\n",
        "- Takes a dictionary mapping old names to new names\n",
        "- Useful for:\n",
        "  - Standardizing column naming conventions\n",
        "  - Simplifying long or unclear column names\n",
        "  - Preparing data for downstream analysis\n",
        "  - Making data more readable and accessible\n",
        "- Returns a new DataFrame with renamed columns\n",
        "\n",
        "**Use Cases:**\n",
        "- Shortening verbose column names\n",
        "- Converting naming conventions (snake_case to camelCase, etc.)\n",
        "- Making column names more domain-friendly\n",
        "- Standardizing across multiple data sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_urPrKQiD9Ef",
        "outputId": "bf5a8de4-8842-4a4a-c4bc-46c29916b69c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------------------+---------------+-------------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+-------------+\n",
            "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|trip_duration|\n",
            "+--------------------+---------------------+---------------+-------------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+-------------+\n",
            "| 2025-01-01 00:18:38|  2025-01-01 00:26:59|              1|          1.6|                 N|         229|         237|           1|       10.0|  3.5|    0.5|       3.0|         0.0|                  1.0|        18.0|                 2.5|        0.0|               0.0|          8.4|\n",
            "| 2025-01-01 00:32:40|  2025-01-01 00:35:13|              1|          0.5|                 N|         236|         237|           1|        5.1|  3.5|    0.5|      2.02|         0.0|                  1.0|       12.12|                 2.5|        0.0|               0.0|          2.6|\n",
            "| 2025-01-01 00:44:04|  2025-01-01 00:46:01|              1|          0.6|                 N|         141|         141|           1|        5.1|  3.5|    0.5|       2.0|         0.0|                  1.0|        12.1|                 2.5|        0.0|               0.0|          2.0|\n",
            "| 2025-01-01 00:14:27|  2025-01-01 00:20:01|              3|         0.52|                 N|         244|         244|           2|        7.2|  1.0|    0.5|       0.0|         0.0|                  1.0|         9.7|                 0.0|        0.0|               0.0|          5.6|\n",
            "| 2025-01-01 00:21:34|  2025-01-01 00:25:06|              3|         0.66|                 N|         244|         116|           2|        5.8|  1.0|    0.5|       0.0|         0.0|                  1.0|         8.3|                 0.0|        0.0|               0.0|          3.5|\n",
            "| 2025-01-01 00:48:24|  2025-01-01 01:08:26|              2|         2.63|                 N|         239|          68|           2|       19.1|  1.0|    0.5|       0.0|         0.0|                  1.0|        24.1|                 2.5|        0.0|               0.0|         20.0|\n",
            "| 2025-01-01 00:14:47|  2025-01-01 00:16:15|              0|          0.4|                 N|         170|         170|           1|        4.4|  3.5|    0.5|      2.35|         0.0|                  1.0|       11.75|                 2.5|        0.0|               0.0|          1.5|\n",
            "| 2025-01-01 00:39:27|  2025-01-01 00:51:51|              0|          1.6|                 N|         234|         148|           1|       12.1|  3.5|    0.5|       2.0|         0.0|                  1.0|        19.1|                 2.5|        0.0|               0.0|         12.4|\n",
            "| 2025-01-01 00:53:43|  2025-01-01 01:13:23|              0|          2.8|                 N|         148|         170|           1|       19.1|  3.5|    0.5|       3.0|         0.0|                  1.0|        27.1|                 2.5|        0.0|               0.0|         19.7|\n",
            "| 2025-01-01 00:00:02|  2025-01-01 00:09:36|              1|         1.71|                 N|         237|         262|           2|       11.4|  1.0|    0.5|       0.0|         0.0|                  1.0|        16.4|                 2.5|        0.0|               0.0|          9.6|\n",
            "| 2025-01-01 00:20:28|  2025-01-01 00:28:04|              1|         2.29|                 N|         237|          75|           2|       11.4|  1.0|    0.5|       0.0|         0.0|                  1.0|        16.4|                 2.5|        0.0|               0.0|          7.6|\n",
            "| 2025-01-01 00:33:58|  2025-01-01 00:37:23|              1|         0.56|                 N|         263|         236|           1|        5.8|  1.0|    0.5|      2.16|         0.0|                  1.0|       12.96|                 2.5|        0.0|               0.0|          3.4|\n",
            "| 2025-01-01 00:42:40|  2025-01-01 00:55:38|              3|         1.99|                 N|         236|         151|           2|       14.2|  1.0|    0.5|       0.0|         0.0|                  1.0|        19.2|                 2.5|        0.0|               0.0|         13.0|\n",
            "| 2025-01-01 00:30:07|  2025-01-01 00:36:48|              1|          1.1|                 N|         229|         141|           2|        7.9|  3.5|    0.5|       0.0|         0.0|                  1.0|        12.9|                 2.5|        0.0|               0.0|          6.7|\n",
            "| 2025-01-01 00:39:55|  2025-01-01 01:13:59|              1|          3.2|                 N|         141|         113|           1|       26.1|  3.5|    0.5|       7.8|         0.0|                  1.0|        38.9|                 2.5|        0.0|               0.0|         34.1|\n",
            "| 2025-01-01 00:16:54|  2025-01-01 00:35:12|              3|          2.5|                 N|         158|         170|           2|       17.7|  3.5|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|               0.0|         18.3|\n",
            "| 2025-01-01 00:43:10|  2025-01-01 01:00:03|              1|          1.9|                 N|         164|         229|           1|       16.3|  3.5|    0.5|      4.25|         0.0|                  1.0|       25.55|                 2.5|        0.0|               0.0|         16.9|\n",
            "| 2025-01-01 00:01:41|  2025-01-01 00:07:14|              1|         0.71|                 N|          79|         107|           2|       -7.2| -1.0|   -0.5|      3.66|         0.0|                 -1.0|       -8.54|                -2.5|        0.0|               0.0|          5.6|\n",
            "| 2025-01-01 00:01:41|  2025-01-01 00:07:14|              1|         0.71|                 N|          79|         107|           2|        7.2|  1.0|    0.5|       0.0|         0.0|                  1.0|        12.2|                 2.5|        0.0|               0.0|          5.6|\n",
            "| 2025-01-01 00:33:12|  2025-01-01 00:50:14|              2|          1.2|                 N|         246|          90|           1|       15.6|  3.5|    0.5|       0.0|         0.0|                  1.0|        20.6|                 2.5|        0.0|               0.0|         17.0|\n",
            "+--------------------+---------------------+---------------+-------------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+-------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# Drop unnecessary columns to clean up the DataFrame\n",
        "# Remove VendorID and RateCodeID columns as they're not needed for analysis\n",
        "df1.drop('VendorID', 'RateCodeID').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXn-GBnP5XUr"
      },
      "source": [
        "---\n",
        "\n",
        "## 15. Dropping Unnecessary Columns\n",
        "\n",
        "**The `drop()` Method:**\n",
        "- Removes one or more columns from a DataFrame\n",
        "- Useful for cleaning up unwanted columns after joins or transformations\n",
        "- Takes column name(s) as arguments\n",
        "- Returns a new DataFrame without the specified columns\n",
        "\n",
        "**Use Cases:**\n",
        "- Remove duplicate columns after joins\n",
        "- Clean up intermediate columns not needed for analysis\n",
        "- Reduce data size by removing non-essential fields\n",
        "- Prepare data for output or downstream processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTLx_ogaE1Io"
      },
      "source": [
        "---\n",
        "\n",
        "## 16. Combining DataFrames: Unions & Joins\n",
        "\n",
        "### Union - Combining Rows\n",
        "\n",
        "**The `union()` Method:**\n",
        "- Combines two DataFrames with the same schema by stacking rows\n",
        "- Creates a single DataFrame from multiple sources\n",
        "- **Does NOT remove duplicates** (use `unionByName()` or distinct() if needed)\n",
        "- Useful for combining data from different time periods or sources\n",
        "\n",
        "**Syntax:**\n",
        "```python\n",
        "combined_df = df1.union(df2)\n",
        "```\n",
        "\n",
        "**Key Points:**\n",
        "- Both DataFrames must have the same columns in the same order\n",
        "- Row count increases: `len(combined_df) = len(df1) + len(df2)`\n",
        "- Duplicates are preserved; filter or use `distinct()` if cleanup needed\n",
        "\n",
        "### Join - Combining Columns\n",
        "\n",
        "**The `join()` Method:**\n",
        "- Combines two DataFrames based on a condition (key)\n",
        "- Similar to SQL joins (INNER, LEFT, RIGHT, OUTER, CROSS)\n",
        "- Creates a new DataFrame with columns from both tables\n",
        "\n",
        "**Syntax:**\n",
        "```python\n",
        "joined_df = df1.join(df2, df1.key_col == df2.key_col, 'left')\n",
        "```\n",
        "\n",
        "**Join Types:**\n",
        "- **inner**: Only matching rows (default)\n",
        "- **left**: All from left table + matching from right\n",
        "- **right**: All from right table + matching from left\n",
        "- **outer**: All rows from both tables\n",
        "- **cross**: Cartesian product (all combinations)\n",
        "\n",
        "**Use Cases:**\n",
        "- Enriching data with lookup tables\n",
        "- Combining related datasets\n",
        "- Merging data from different sources\n",
        "- Creating complex analytical datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "WxEptrg5D9TE"
      },
      "outputs": [],
      "source": [
        "# Load February data to combine with January data\n",
        "# We'll use union to combine datasets from different months\n",
        "df_feb = spark.read.parquet('/content/yellow_tripdata_2025-02.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "-hJSF6LvD9Wh"
      },
      "outputs": [],
      "source": [
        "# Combine January and February data using union()\n",
        "# This stacks rows from both DataFrames vertically\n",
        "df_2025_combined = df.union(df_feb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xllD6KX5D9Zy",
        "outputId": "ac7eae6d-4215-4ea0-ff3b-ceb25aaddabf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7052769"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "# Count the combined records\n",
        "# Note: union() preserves duplicates, so row count = jan_count + feb_count\n",
        "df_2025_combined.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "Inlf12FUD9h0"
      },
      "outputs": [],
      "source": [
        "# Load taxi zone lookup data for enrichment\n",
        "# This will be joined with trip data to add location information\n",
        "taxi_zone_lookup = spark.read.option('header', 'true').csv('/content/taxi_zone_lookup.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8EdgE-wD9lX",
        "outputId": "7b3a58a8-cba4-4758-a076-224fdeaaeee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+--------------------+------------+\n",
            "|LocationID|      Borough|                Zone|service_zone|\n",
            "+----------+-------------+--------------------+------------+\n",
            "|         1|          EWR|      Newark Airport|         EWR|\n",
            "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
            "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
            "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
            "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
            "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
            "|         7|       Queens|             Astoria|   Boro Zone|\n",
            "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
            "|         9|       Queens|          Auburndale|   Boro Zone|\n",
            "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
            "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
            "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
            "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
            "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
            "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
            "|        16|       Queens|             Bayside|   Boro Zone|\n",
            "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
            "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
            "|        19|       Queens|           Bellerose|   Boro Zone|\n",
            "|        20|        Bronx|             Belmont|   Boro Zone|\n",
            "+----------+-------------+--------------------+------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# Preview the taxi zone lookup table structure\n",
        "# Contains LocationID, Borough, Zone, and service_zone information\n",
        "taxi_zone_lookup.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oWEIiVsGxz2",
        "outputId": "2818557c-5ecb-4efc-e404-2a06e04e3e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+----------+---------+--------------------+------------+\n",
            "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|LocationID|  Borough|                Zone|service_zone|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+----------+---------+--------------------+------------+\n",
            "|       1| 2025-01-01 00:18:38|  2025-01-01 00:26:59|              1|          1.6|         1|                 N|         229|         237|           1|       10.0|  3.5|    0.5|       3.0|         0.0|                  1.0|        18.0|                 2.5|        0.0|               0.0|       229|Manhattan|Sutton Place/Turt...| Yellow Zone|\n",
            "|       1| 2025-01-01 00:32:40|  2025-01-01 00:35:13|              1|          0.5|         1|                 N|         236|         237|           1|        5.1|  3.5|    0.5|      2.02|         0.0|                  1.0|       12.12|                 2.5|        0.0|               0.0|       236|Manhattan|Upper East Side N...| Yellow Zone|\n",
            "|       1| 2025-01-01 00:44:04|  2025-01-01 00:46:01|              1|          0.6|         1|                 N|         141|         141|           1|        5.1|  3.5|    0.5|       2.0|         0.0|                  1.0|        12.1|                 2.5|        0.0|               0.0|       141|Manhattan|     Lenox Hill West| Yellow Zone|\n",
            "|       2| 2025-01-01 00:14:27|  2025-01-01 00:20:01|              3|         0.52|         1|                 N|         244|         244|           2|        7.2|  1.0|    0.5|       0.0|         0.0|                  1.0|         9.7|                 0.0|        0.0|               0.0|       244|Manhattan|Washington Height...|   Boro Zone|\n",
            "|       2| 2025-01-01 00:21:34|  2025-01-01 00:25:06|              3|         0.66|         1|                 N|         244|         116|           2|        5.8|  1.0|    0.5|       0.0|         0.0|                  1.0|         8.3|                 0.0|        0.0|               0.0|       244|Manhattan|Washington Height...|   Boro Zone|\n",
            "|       2| 2025-01-01 00:48:24|  2025-01-01 01:08:26|              2|         2.63|         1|                 N|         239|          68|           2|       19.1|  1.0|    0.5|       0.0|         0.0|                  1.0|        24.1|                 2.5|        0.0|               0.0|       239|Manhattan|Upper West Side S...| Yellow Zone|\n",
            "|       1| 2025-01-01 00:14:47|  2025-01-01 00:16:15|              0|          0.4|         1|                 N|         170|         170|           1|        4.4|  3.5|    0.5|      2.35|         0.0|                  1.0|       11.75|                 2.5|        0.0|               0.0|       170|Manhattan|         Murray Hill| Yellow Zone|\n",
            "|       1| 2025-01-01 00:39:27|  2025-01-01 00:51:51|              0|          1.6|         1|                 N|         234|         148|           1|       12.1|  3.5|    0.5|       2.0|         0.0|                  1.0|        19.1|                 2.5|        0.0|               0.0|       234|Manhattan|            Union Sq| Yellow Zone|\n",
            "|       1| 2025-01-01 00:53:43|  2025-01-01 01:13:23|              0|          2.8|         1|                 N|         148|         170|           1|       19.1|  3.5|    0.5|       3.0|         0.0|                  1.0|        27.1|                 2.5|        0.0|               0.0|       148|Manhattan|     Lower East Side| Yellow Zone|\n",
            "|       2| 2025-01-01 00:00:02|  2025-01-01 00:09:36|              1|         1.71|         1|                 N|         237|         262|           2|       11.4|  1.0|    0.5|       0.0|         0.0|                  1.0|        16.4|                 2.5|        0.0|               0.0|       237|Manhattan|Upper East Side S...| Yellow Zone|\n",
            "|       2| 2025-01-01 00:20:28|  2025-01-01 00:28:04|              1|         2.29|         1|                 N|         237|          75|           2|       11.4|  1.0|    0.5|       0.0|         0.0|                  1.0|        16.4|                 2.5|        0.0|               0.0|       237|Manhattan|Upper East Side S...| Yellow Zone|\n",
            "|       2| 2025-01-01 00:33:58|  2025-01-01 00:37:23|              1|         0.56|         1|                 N|         263|         236|           1|        5.8|  1.0|    0.5|      2.16|         0.0|                  1.0|       12.96|                 2.5|        0.0|               0.0|       263|Manhattan|      Yorkville West| Yellow Zone|\n",
            "|       2| 2025-01-01 00:42:40|  2025-01-01 00:55:38|              3|         1.99|         1|                 N|         236|         151|           2|       14.2|  1.0|    0.5|       0.0|         0.0|                  1.0|        19.2|                 2.5|        0.0|               0.0|       236|Manhattan|Upper East Side N...| Yellow Zone|\n",
            "|       1| 2025-01-01 00:30:07|  2025-01-01 00:36:48|              1|          1.1|         1|                 N|         229|         141|           2|        7.9|  3.5|    0.5|       0.0|         0.0|                  1.0|        12.9|                 2.5|        0.0|               0.0|       229|Manhattan|Sutton Place/Turt...| Yellow Zone|\n",
            "|       1| 2025-01-01 00:39:55|  2025-01-01 01:13:59|              1|          3.2|         1|                 N|         141|         113|           1|       26.1|  3.5|    0.5|       7.8|         0.0|                  1.0|        38.9|                 2.5|        0.0|               0.0|       141|Manhattan|     Lenox Hill West| Yellow Zone|\n",
            "|       1| 2025-01-01 00:16:54|  2025-01-01 00:35:12|              3|          2.5|         1|                 N|         158|         170|           2|       17.7|  3.5|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|               0.0|       158|Manhattan|Meatpacking/West ...| Yellow Zone|\n",
            "|       1| 2025-01-01 00:43:10|  2025-01-01 01:00:03|              1|          1.9|         1|                 N|         164|         229|           1|       16.3|  3.5|    0.5|      4.25|         0.0|                  1.0|       25.55|                 2.5|        0.0|               0.0|       164|Manhattan|       Midtown South| Yellow Zone|\n",
            "|       2| 2025-01-01 00:01:41|  2025-01-01 00:07:14|              1|         0.71|         1|                 N|          79|         107|           2|       -7.2| -1.0|   -0.5|      3.66|         0.0|                 -1.0|       -8.54|                -2.5|        0.0|               0.0|        79|Manhattan|        East Village| Yellow Zone|\n",
            "|       2| 2025-01-01 00:01:41|  2025-01-01 00:07:14|              1|         0.71|         1|                 N|          79|         107|           2|        7.2|  1.0|    0.5|       0.0|         0.0|                  1.0|        12.2|                 2.5|        0.0|               0.0|        79|Manhattan|        East Village| Yellow Zone|\n",
            "|       1| 2025-01-01 00:33:12|  2025-01-01 00:50:14|              2|          1.2|         1|                 N|         246|          90|           1|       15.6|  3.5|    0.5|       0.0|         0.0|                  1.0|        20.6|                 2.5|        0.0|               0.0|       246|Manhattan|West Chelsea/Huds...| Yellow Zone|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+----------+---------+--------------------+------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# Join combined trip data with zone lookup using LEFT JOIN\n",
        "# This adds location information to each trip based on pickup location\n",
        "df_joined = df_2025_combined.join(taxi_zone_lookup,\n",
        "                                   df_2025_combined.PULocationID == taxi_zone_lookup.LocationID,\n",
        "                                   'left')\n",
        "df_joined.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLgvIGi4GyAX",
        "outputId": "149da5b8-b011-4e62-9aa5-0a10fcb44ee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------+\n",
            "|payment_type|  count|\n",
            "+------------+-------+\n",
            "|           0|1347086|\n",
            "|           1|4780568|\n",
            "|           2| 729910|\n",
            "|           3|  45622|\n",
            "|           4| 149582|\n",
            "|           5|      1|\n",
            "+------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Group by payment type and count records in each group\n",
        "# Then sort by payment_type for consistent output\n",
        "payment_type_counts = df_2025_combined.groupby('payment_type').count().sort('payment_type')\n",
        "payment_type_counts.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm-DsjQt5XUu"
      },
      "source": [
        "---\n",
        "\n",
        "## 17. Grouping and Aggregation\n",
        "\n",
        "### The `groupBy()` and `agg()` Methods\n",
        "\n",
        "**Grouping Data:**\n",
        "- `groupBy()` divides data into groups based on column values\n",
        "- Must be followed by an aggregation function\n",
        "- Returns aggregated results (not raw grouped data)\n",
        "\n",
        "**Common Aggregation Functions:**\n",
        "- `count()` - Number of records per group\n",
        "- `sum()` - Total value per group\n",
        "- `avg()` - Average value per group\n",
        "- `min()` / `max()` - Minimum/Maximum values\n",
        "- `stddev()` - Standard deviation\n",
        "- `agg()` - Apply multiple aggregations at once\n",
        "\n",
        "**Use Cases:**\n",
        "- Summary statistics by category\n",
        "- Identifying patterns in grouped data\n",
        "- Business metrics by dimension (e.g., sales by region)\n",
        "- Data validation (e.g., record counts per day)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvNHyezMGyMi",
        "outputId": "774dcba5-9d80-4475-f0ff-1e64e1917ec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------------+\n",
            "|payment_type| avg(total_amount)|\n",
            "+------------+------------------+\n",
            "|           5|               0.0|\n",
            "|           1| 28.01570129324061|\n",
            "|           3| 9.396843628074157|\n",
            "|           2|  21.5920279349519|\n",
            "|           4| 6.355892152799148|\n",
            "|           0|20.401269013263903|\n",
            "+------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate average fare amount by payment type\n",
        "payment_type_avg_fare = df_2025_combined.groupby('payment_type').avg('total_amount')\n",
        "payment_type_avg_fare.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmTgrNA_GyXN",
        "outputId": "fea6fec0-7b03-43c2-fc99-0041923eb788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------------+\n",
            "|payment_type|   avg_fare_amount|\n",
            "+------------+------------------+\n",
            "|           5|               0.0|\n",
            "|           1| 28.01570129324061|\n",
            "|           3| 9.396843628074157|\n",
            "|           2|  21.5920279349519|\n",
            "|           4| 6.355892152799148|\n",
            "|           0|20.401269013263903|\n",
            "+------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import aggregation function for custom naming\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Calculate average total_amount and alias the result column\n",
        "# Using agg() with alias allows for more flexible aggregations\n",
        "payment_type_avg_fare_named = df_2025_combined.groupby('payment_type').agg(avg('total_amount').alias('avg_fare_amount'))\n",
        "payment_type_avg_fare_named.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "j5vsVCKsGyfW"
      },
      "outputs": [],
      "source": [
        "# Calculate and persist average fare by payment type\n",
        "# Aggregate by payment_type, calculate average, and save results\n",
        "avg_fare_by_payment = df_2025_combined.groupBy('payment_type').agg(avg('total_amount')).sort('payment_type')\n",
        "\n",
        "# Write aggregated results to CSV for reporting or further analysis\n",
        "avg_fare_by_payment.write.csv('/content/avg_fare_by_payment', header=True, mode='overwrite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItJNBVd2NdXx"
      },
      "source": [
        "---\n",
        "\n",
        "## 18. Challenge Exercise: DataFrame Operations\n",
        "\n",
        "**Objective:** Combine multiple PySpark concepts to create a comprehensive data pipeline\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. **Load Data**: Create two DataFrames (df_jan and df_feb) from January and February taxi data\n",
        "2. **Union**: Combine them into df_2025_combined using union()\n",
        "3. **Transform Columns**: Select relevant columns and rename for clarity:\n",
        "   - `tpep_pickup_datetime`  `pu_datetime`\n",
        "   - `tpep_dropoff_datetime`  `do_datetime`\n",
        "   - `PULocationID`  `pu_location_id`\n",
        "   - `DOLocationID`  `do_location_id`\n",
        "   - `Airport_fee`  `airport_fee`\n",
        "   \n",
        "\n",
        "4. **Join**: Load taxi zone lookup and LEFT JOIN to add borough information\n",
        "5. **Clean**: Drop duplicate/unnecessary columns (LocationID, zone, service_zone)\n",
        "6. **Rename**: Rename 'Borough' to 'pu_boro'\n",
        "7. **Output**: Display the result and save to CSV\n",
        "\n",
        "**Skills Practiced:**\n",
        "- Reading multiple data sources\n",
        "- Union operations\n",
        "- Column selection and renaming\n",
        "- Join operations\n",
        "- Data cleanup\n",
        "- CSV output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnEOZhBYfsHY",
        "outputId": "16258c57-a8ce-4e42-d597-048efd2faced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------------------+--------------+--------------+---------------+-----------+-----------+------------+------------+-------------+--------+---------+\n",
            "|        pu_datetime|        do_datetime|pu_location_id|do_location_id|passenger_count|fare_amount|airport_fee|total_amount|payment_type|trip_distance|VendorID|  pu_boro|\n",
            "+-------------------+-------------------+--------------+--------------+---------------+-----------+-----------+------------+------------+-------------+--------+---------+\n",
            "|2025-01-01 00:18:38|2025-01-01 00:26:59|           229|           237|              1|       10.0|        0.0|        18.0|           1|          1.6|       1|Manhattan|\n",
            "|2025-01-01 00:32:40|2025-01-01 00:35:13|           236|           237|              1|        5.1|        0.0|       12.12|           1|          0.5|       1|Manhattan|\n",
            "|2025-01-01 00:44:04|2025-01-01 00:46:01|           141|           141|              1|        5.1|        0.0|        12.1|           1|          0.6|       1|Manhattan|\n",
            "|2025-01-01 00:14:27|2025-01-01 00:20:01|           244|           244|              3|        7.2|        0.0|         9.7|           2|         0.52|       2|Manhattan|\n",
            "|2025-01-01 00:21:34|2025-01-01 00:25:06|           244|           116|              3|        5.8|        0.0|         8.3|           2|         0.66|       2|Manhattan|\n",
            "|2025-01-01 00:48:24|2025-01-01 01:08:26|           239|            68|              2|       19.1|        0.0|        24.1|           2|         2.63|       2|Manhattan|\n",
            "|2025-01-01 00:14:47|2025-01-01 00:16:15|           170|           170|              0|        4.4|        0.0|       11.75|           1|          0.4|       1|Manhattan|\n",
            "|2025-01-01 00:39:27|2025-01-01 00:51:51|           234|           148|              0|       12.1|        0.0|        19.1|           1|          1.6|       1|Manhattan|\n",
            "|2025-01-01 00:53:43|2025-01-01 01:13:23|           148|           170|              0|       19.1|        0.0|        27.1|           1|          2.8|       1|Manhattan|\n",
            "|2025-01-01 00:00:02|2025-01-01 00:09:36|           237|           262|              1|       11.4|        0.0|        16.4|           2|         1.71|       2|Manhattan|\n",
            "|2025-01-01 00:20:28|2025-01-01 00:28:04|           237|            75|              1|       11.4|        0.0|        16.4|           2|         2.29|       2|Manhattan|\n",
            "|2025-01-01 00:33:58|2025-01-01 00:37:23|           263|           236|              1|        5.8|        0.0|       12.96|           1|         0.56|       2|Manhattan|\n",
            "|2025-01-01 00:42:40|2025-01-01 00:55:38|           236|           151|              3|       14.2|        0.0|        19.2|           2|         1.99|       2|Manhattan|\n",
            "|2025-01-01 00:30:07|2025-01-01 00:36:48|           229|           141|              1|        7.9|        0.0|        12.9|           2|          1.1|       1|Manhattan|\n",
            "|2025-01-01 00:39:55|2025-01-01 01:13:59|           141|           113|              1|       26.1|        0.0|        38.9|           1|          3.2|       1|Manhattan|\n",
            "|2025-01-01 00:16:54|2025-01-01 00:35:12|           158|           170|              3|       17.7|        0.0|        22.7|           2|          2.5|       1|Manhattan|\n",
            "|2025-01-01 00:43:10|2025-01-01 01:00:03|           164|           229|              1|       16.3|        0.0|       25.55|           1|          1.9|       1|Manhattan|\n",
            "|2025-01-01 00:01:41|2025-01-01 00:07:14|            79|           107|              1|       -7.2|        0.0|       -8.54|           2|         0.71|       2|Manhattan|\n",
            "|2025-01-01 00:01:41|2025-01-01 00:07:14|            79|           107|              1|        7.2|        0.0|        12.2|           2|         0.71|       2|Manhattan|\n",
            "|2025-01-01 00:33:12|2025-01-01 00:50:14|           246|            90|              2|       15.6|        0.0|        20.6|           1|          1.2|       1|Manhattan|\n",
            "+-------------------+-------------------+--------------+--------------+---------------+-----------+-----------+------------+------------+-------------+--------+---------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# Challenge Solution: Complete data pipeline\n",
        "# Step 1: Load January and February data\n",
        "df_jan = spark.read.parquet('/content/yellow_tripdata_2025-01.parquet')\n",
        "df_feb = spark.read.parquet('/content/yellow_tripdata_2025-02.parquet')\n",
        "\n",
        "# Step 2: Combine using union\n",
        "df_2025_combined = df_jan.union(df_feb)\n",
        "\n",
        "# Step 3: Select and rename columns for clarity and consistency\n",
        "df_2025_combined = df_2025_combined\\\n",
        "                      .select('tpep_pickup_datetime','tpep_dropoff_datetime', \\\n",
        "                              'PULocationID', 'DOLocationID', 'passenger_count',\\\n",
        "                              'fare_amount', 'Airport_fee', 'total_amount', 'payment_type','trip_distance','VendorID')\\\n",
        "                      .withColumnsRenamed({'tpep_pickup_datetime':'pu_datetime',\\\n",
        "                              'tpep_dropoff_datetime':'do_datetime',\\\n",
        "                              'PULocationID':'pu_location_id',\\\n",
        "                              'DOLocationID':'do_location_id',\\\n",
        "                              'Airport_fee': 'airport_fee'})\n",
        "\n",
        "# Step 4: Load zone lookup and join for location enrichment\n",
        "taxi_zones = spark.read.option('header','true').csv('/content/taxi_zone_lookup.csv')\n",
        "\n",
        "# Left join to add borough information\n",
        "df_2025_combined = df_2025_combined.join(taxi_zones,\n",
        "                                          taxi_zones.LocationID == df_2025_combined.do_location_id,\n",
        "                                          'left')\n",
        "\n",
        "# Step 5 & 6: Drop superfluous columns and rename Borough\n",
        "df_2025_combined = df_2025_combined.drop('LocationID','zone','service_zone')\\\n",
        "                                     .withColumnsRenamed({'Borough':'pu_boro'})\n",
        "\n",
        "# Step 7: Write results to CSV and display\n",
        "df_2025_combined.write.csv(\"/content/2025_combined\", header=True, mode='overwrite')\n",
        "df_2025_combined.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfGWLfQuRv9l"
      },
      "source": [
        "---\n",
        "\n",
        "## 19. PySpark SQL - SQL Interface for DataFrames\n",
        "\n",
        "**What is Spark SQL?**\n",
        "- Enables writing SQL queries directly on Spark DataFrames\n",
        "- Combines the power of SQL with distributed computing\n",
        "- Familiar syntax for SQL users\n",
        "- Often optimized better than DataFrame API by Spark's Catalyst optimizer\n",
        "\n",
        "**How to Use:**\n",
        "1. Create a temporary view from a DataFrame: `df.createOrReplaceTempView('table_name')`\n",
        "2. Query using SQL: `spark.sql('SELECT ...')`\n",
        "3. Can combine with DataFrame operations\n",
        "\n",
        "**Benefits:**\n",
        "- Easy for SQL-familiar users\n",
        "- Complex queries can be more readable\n",
        "- Leverage SQL optimization engine\n",
        "- Join DataFrames using SQL syntax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "T0FB209LGysP"
      },
      "outputs": [],
      "source": [
        "# Load taxi data for SQL operations\n",
        "taxi_df = spark.read.parquet('/content/yellow_tripdata_2025-01.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "rkdOGXYUGyzw"
      },
      "outputs": [],
      "source": [
        "# Register the DataFrame as a temporary SQL view\n",
        "# This allows SQL queries to be run on the data\n",
        "taxi_df.createOrReplaceTempView('taxi')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfYBCeMESD1n",
        "outputId": "58658417-3eac-411f-bd2c-62f58cc63452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "|       2| 2025-01-01 00:15:41|  2025-01-01 01:03:03|              4|         3.05|         1|                 N|         114|         161|           1|       37.3|  1.0|    0.5|      8.46|         0.0|                  1.0|       50.76|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:34:23|  2025-01-01 01:20:57|              1|        10.42|         1|                 N|          90|         243|           1|       56.9|  1.0|    0.5|     12.38|         0.0|                  1.0|       74.28|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:55:44|  2025-01-01 01:27:41|              1|        14.84|         1|                 N|         132|          89|           1|       59.0|  1.0|    0.5|      12.3|         0.0|                  1.0|       75.55|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:04:29|  2025-01-01 00:55:58|              9|        31.97|         5|                 N|         132|         265|           2|       90.0|  0.0|    0.0|       0.0|       20.32|                  1.0|      111.32|                 0.0|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:44:59|  2025-01-01 01:23:14|              1|         10.9|         1|                 N|         261|          42|           1|       47.1|  3.5|    0.5|       5.0|         0.0|                  1.0|        57.1|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:11:59|  2025-01-01 00:48:37|              2|         9.17|         1|                 N|         138|         186|           1|       48.5|  6.0|    0.5|     13.09|        6.94|                  1.0|       80.28|                 2.5|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:39:59|  2025-01-01 01:00:37|              1|         7.78|         1|                 N|         138|          36|           1|       33.8|  6.0|    0.5|      8.26|         0.0|                  1.0|       51.31|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:48:53|  2025-01-01 01:13:31|              1|        11.81|         5|                 N|          68|         265|           1|       80.0|  0.0|    0.0|       0.0|       21.38|                  1.0|      102.38|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:32:29|  2025-01-01 01:09:35|              1|         9.79|         1|                 N|         239|          33|           1|       44.3|  1.0|    0.5|     12.32|         0.0|                  1.0|       61.62|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:00:00|  2025-01-01 01:03:09|              1|          6.4|         1|                 Y|         211|         164|           1|       52.0|  3.5|    0.5|     17.05|         0.0|                  1.0|       74.05|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:24:04|  2025-01-01 01:09:28|              3|         5.55|         1|                 N|         238|         113|           1|       40.1|  1.0|    0.5|     11.28|         0.0|                  1.0|       56.38|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:17:24|  2025-01-01 00:56:58|              1|         4.83|         1|                 N|          48|         261|           1|       37.3|  1.0|    0.5|      8.46|         0.0|                  1.0|       50.76|                 2.5|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:38:06|  2025-01-01 01:07:49|              4|          9.4|         1|                 N|          33|         149|           1|       40.8|  1.0|    0.5|      15.0|         0.0|                  1.0|        58.3|                 0.0|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:03:00|  2025-01-01 00:25:10|              2|         10.4|         1|                 N|         138|         162|           1|       40.8|10.25|    0.5|       0.0|        6.94|                  1.0|       59.49|                 2.5|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:07:59|  2025-01-01 00:36:49|              1|        15.18|         1|                 N|         161|         200|           1|       59.7|  1.0|    0.5|     12.94|         0.0|                  1.0|       77.64|                 2.5|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:52:40|  2025-01-01 01:21:14|              1|        18.57|         1|                 N|          93|          29|           1|       68.8|  1.0|    0.5|      12.0|         0.0|                  1.0|        83.3|                 0.0|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:26:47|  2025-01-01 01:10:36|              1|         14.8|        99|                 N|          41|         122|           1|       47.5|  0.0|    0.5|       0.0|        6.94|                  1.0|       55.94|                 0.0|        0.0|               0.0|\n",
            "|       1| 2025-01-01 00:23:05|  2025-01-01 00:56:38|              1|         22.0|         1|                 N|         132|         241|           2|       80.7| 2.75|    0.5|       0.0|        6.94|                  1.0|       91.89|                 0.0|       1.75|               0.0|\n",
            "|       2| 2025-01-01 00:26:09|  2025-01-01 00:51:09|              1|         5.69|         5|                 N|         186|         265|           1|       60.0|  0.0|    0.0|     14.88|       13.38|                  1.0|       89.26|                 0.0|        0.0|               0.0|\n",
            "|       2| 2025-01-01 00:16:32|  2025-01-01 00:47:36|              2|        16.43|         2|                 N|         170|         219|           1|       70.0|  0.0|    0.5|      10.0|        6.94|                  1.0|       90.94|                 2.5|        0.0|               0.0|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# Simple SQL query: Select all trips with high fare amounts\n",
        "# This demonstrates basic WHERE clause filtering in SQL\n",
        "spark.sql('SELECT * FROM taxi WHERE total_amount > 50').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuABUxieSD9A",
        "outputId": "f9b11bb7-0ba9-456a-f388-20e3d6f1da32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+---------------+------------+\n",
            "|payment_type|passenger_count|total_amount|\n",
            "+------------+---------------+------------+\n",
            "|           1|              4|       50.76|\n",
            "|           2|              9|      111.32|\n",
            "|           1|              3|       56.38|\n",
            "|           1|              4|        58.3|\n",
            "|           1|              3|       51.55|\n",
            "|           1|              4|       91.19|\n",
            "|           2|              3|        59.1|\n",
            "|           1|              4|       61.27|\n",
            "|           3|              4|      123.44|\n",
            "|           1|              3|       59.09|\n",
            "|           2|              3|       192.4|\n",
            "|           1|              4|       62.52|\n",
            "|           1|              3|      151.35|\n",
            "|           1|              3|       88.92|\n",
            "|           1|              4|       100.0|\n",
            "|           1|              4|      115.05|\n",
            "|           2|              4|       80.94|\n",
            "|           1|              3|        58.1|\n",
            "|           1|              3|       62.49|\n",
            "|           1|              3|       71.24|\n",
            "+------------+---------------+------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# Combining SQL queries with DataFrame operations\n",
        "# SQL query provides initial filtering, then DataFrame method refines results\n",
        "high_fare_trips = spark.sql('SELECT * FROM taxi WHERE total_amount > 50')\\\n",
        ".filter('passenger_count > 2')\\\n",
        ".select('payment_type','passenger_count','total_amount')\\\n",
        ".show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8P0JRw0SECH",
        "outputId": "8c0e3b13-d11f-4ad4-956f-155b52c334c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+---------------+------------+\n",
            "|payment_type|passenger_count|total_amount|\n",
            "+------------+---------------+------------+\n",
            "|           1|              4|       50.76|\n",
            "|           2|              9|      111.32|\n",
            "|           1|              3|       56.38|\n",
            "|           1|              4|        58.3|\n",
            "|           1|              3|       51.55|\n",
            "|           1|              4|       91.19|\n",
            "|           2|              3|        59.1|\n",
            "|           1|              4|       61.27|\n",
            "|           3|              4|      123.44|\n",
            "|           1|              3|       59.09|\n",
            "|           2|              3|       192.4|\n",
            "|           1|              4|       62.52|\n",
            "|           1|              3|      151.35|\n",
            "|           1|              3|       88.92|\n",
            "|           1|              4|       100.0|\n",
            "|           1|              4|      115.05|\n",
            "|           2|              4|       80.94|\n",
            "|           1|              3|        58.1|\n",
            "|           1|              3|       62.49|\n",
            "|           1|              3|       71.24|\n",
            "+------------+---------------+------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "# More complex SQL query with multiple conditions\n",
        "# Using multi-line SQL for better readability\n",
        "complex_query = \"\"\"\n",
        "SELECT payment_type, passenger_count, total_amount\n",
        "FROM taxi\n",
        "WHERE\n",
        "  total_amount > 50\n",
        "  AND\n",
        "  passenger_count > 2\n",
        "\"\"\"\n",
        "spark.sql(complex_query).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDGw4EB7Uo-F"
      },
      "source": [
        "---\n",
        "\n",
        "## 20. Challenge Exercise: SQL and Aggregation\n",
        "\n",
        "**Objective:** Combine SQL queries with aggregation and joins to answer business questions\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. **Load Data**: Load January taxi data and register as a temp view called 'taxi'\n",
        "2. **Load Lookup**: Load taxi zone lookup CSV and register as 'taxi_lookup'\n",
        "3. **SQL Join**: Use SQL to LEFT JOIN taxi data with zone lookup on DOLocationID\n",
        "   - Select: DOLocationID, Borough, total_amount\n",
        "   - Assign result to `joined_df`\n",
        "4. **Aggregation**: Group by Borough and calculate average total_amount\n",
        "   - Alias the average column as `avg_amount`\n",
        "5. **Output**: Display results and save to CSV\n",
        "\n",
        "**Business Question:** What is the average fare amount by borough for dropoff locations?\n",
        "\n",
        "**Skills Practiced:**\n",
        "- Creating temporary SQL views\n",
        "- SQL JOIN operations\n",
        "- DataFrame grouping and aggregation\n",
        "- Aliasing columns for clarity\n",
        "- CSV export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "WFY_QTgASEGH"
      },
      "outputs": [],
      "source": [
        "# Challenge Solution: SQL Join + Aggregation\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Step 1: Load and register taxi data\n",
        "taxi_df = spark.read.parquet('/content/yellow_tripdata_2025-01.parquet')\n",
        "taxi_df.createOrReplaceTempView('taxi')\n",
        "\n",
        "# Step 2: Load and register zone lookup\n",
        "taxi_zone_lookup = spark.read.csv('/content/taxi_zone_lookup.csv', header=True, mode='overwrite')\n",
        "taxi_zone_lookup.createOrReplaceTempView('taxi_lookup')\n",
        "\n",
        "# Step 3: SQL JOIN query - combine trip data with zone information\n",
        "join_query = \"\"\"\n",
        " SELECT DOLocationID, Borough, total_amount\n",
        " FROM taxi\n",
        " LEFT JOIN taxi_lookup\n",
        " ON taxi.DOLocationID = taxi_lookup.LocationID\n",
        " \"\"\"\n",
        "joined_df = spark.sql(join_query)\n",
        "\n",
        "# Step 4: Group by Borough and calculate average fare amount\n",
        "avg_fare_by_borough = joined_df.groupBy('Borough').agg(avg('total_amount').alias('avg_amount'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRPBpp93Ygks",
        "outputId": "ddf11073-d4c7-4c67-ad3e-94db65d0545c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------------------+\n",
            "|      Borough|        avg_amount|\n",
            "+-------------+------------------+\n",
            "|       Queens| 51.61163036538719|\n",
            "|          EWR|123.16705659828357|\n",
            "|      Unknown|25.926336005344027|\n",
            "|     Brooklyn| 42.40119152829651|\n",
            "|Staten Island| 88.12902995720401|\n",
            "|          N/A|107.62149760052951|\n",
            "|    Manhattan|22.818652849443495|\n",
            "|        Bronx| 42.89454769447338|\n",
            "+-------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Display results\n",
        "# Shows average fare amount by borough\n",
        "avg_fare_by_borough.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "zVynz1HpYgsP"
      },
      "outputs": [],
      "source": [
        "# Step 6: Export results to CSV for reporting and analysis\n",
        "avg_fare_by_borough.write.option('header', True).mode('overwrite').csv('/content/avg_fare_by_borough')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ameH6tEWbwm_"
      },
      "source": [
        "---\n",
        "\n",
        "## 21. Production Environment Requirements\n",
        "\n",
        "When deploying PySpark applications in production, consider:\n",
        "\n",
        "### Core Requirements\n",
        "\n",
        "**Scalability**\n",
        "- Horizontal scaling with cluster management\n",
        "- Load balancing across nodes\n",
        "- Efficient resource allocation\n",
        "\n",
        "**Reliability**\n",
        "- Fault tolerance and recovery mechanisms\n",
        "- Data consistency and integrity\n",
        "- Error handling and logging\n",
        "\n",
        "**Security**\n",
        "- Access control and authentication\n",
        "- Data encryption (in transit and at rest)\n",
        "- Audit trails and monitoring\n",
        "\n",
        "### Infrastructure Components\n",
        "\n",
        "- **Data Sources**: HDFS, S3, databases, data lakes\n",
        "- **Distributed Storage**: For persistent data, checkpoints, and logs\n",
        "- **Cluster Management**: YARN, Kubernetes, Mesos for orchestration\n",
        "- **Job Scheduling**: Airflow, Oozie, or cloud-native schedulers\n",
        "- **Consistent Environments**: Docker, containerization for reproducibility\n",
        "- **Monitoring & Logging**: Tools like Prometheus, ELK Stack, CloudWatch\n",
        "- **Security & Access Control**: Role-based access, encryption, VPNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji0L7V-b5XU0"
      },
      "source": [
        "---\n",
        "\n",
        "## 22. Advanced Topic: Writing Data with Partitioning\n",
        "\n",
        "**What is Partitioning?**\n",
        "- Divides data across multiple files/directories based on column values\n",
        "- Improves query performance by filtering partitions\n",
        "- Creates physical structure on disk\n",
        "- Enables faster reads when filtering on partition columns\n",
        "\n",
        "**Partitioning Strategies:**\n",
        "1. **Single Column**: Partition by date, region, type\n",
        "2. **Multiple Columns**: Partition by year, month, day for hierarchical structure\n",
        "3. **Range Partitioning**: By numeric ranges\n",
        "4. **Hash Partitioning**: Distribute by hash of column\n",
        "\n",
        "**Benefits:**\n",
        "- Faster queries on partitioned columns\n",
        "- Easier data management and retention\n",
        "- Better parallelization\n",
        "- Reduced data scanning\n",
        "\n",
        "**Trade-offs:**\n",
        "- Increased number of files\n",
        "- Partition pruning requires filter conditions\n",
        "- Uneven partitions can cause skew"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSEdsS8H5XU1",
        "outputId": "6ea6ac3f-a8c0-45c0-ee8d-98a766ea4e96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data written to: /content/taxi_data_partitioned_by_payment\n",
            "Directory structure:\n",
            "  /payment_type=1/\n",
            "  /payment_type=2/\n",
            "  /payment_type=3/\n",
            "  /payment_type=4/\n"
          ]
        }
      ],
      "source": [
        "# Example: Write data partitioned by payment type\n",
        "# This creates separate directories for each payment type, improving query performance\n",
        "\n",
        "df_2025_combined.write \\\n",
        "    .partitionBy('payment_type') \\\n",
        "    .mode('overwrite') \\\n",
        "    .parquet('/content/taxi_data_partitioned_by_payment')\n",
        "\n",
        "print(\"Data written to: /content/taxi_data_partitioned_by_payment\")\n",
        "print(\"Directory structure:\")\n",
        "print(\"  /payment_type=1/\")\n",
        "print(\"  /payment_type=2/\")\n",
        "print(\"  /payment_type=3/\")\n",
        "print(\"  /payment_type=4/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VL1ARPx-5XU0",
        "outputId": "a469756b-4654-4dc4-c519-7d644a05c669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-level partitioned data written!\n",
            "Directory structure (example):\n",
            "  /payment_type=1/trip_month=1/\n",
            "  /payment_type=1/trip_month=2/\n",
            "  /payment_type=2/trip_month=1/\n",
            "  /payment_type=2/trip_month=2/\n"
          ]
        }
      ],
      "source": [
        "# Example: Multi-level Partitioning - by payment type and vendor\n",
        "# Creates hierarchical structure for better organization\n",
        "\n",
        "from pyspark.sql.functions import month\n",
        "\n",
        "df_with_month = df_2025_combined.withColumn('trip_month', month('pu_datetime'))\n",
        "\n",
        "df_with_month.write \\\n",
        "    .partitionBy('payment_type', 'trip_month') \\\n",
        "    .mode('overwrite') \\\n",
        "    .parquet('/content/taxi_data_partitioned_multi')\n",
        "\n",
        "print(\"Multi-level partitioned data written!\")\n",
        "print(\"Directory structure (example):\")\n",
        "print(\"  /payment_type=1/trip_month=1/\")\n",
        "print(\"  /payment_type=1/trip_month=2/\")\n",
        "print(\"  /payment_type=2/trip_month=1/\")\n",
        "print(\"  /payment_type=2/trip_month=2/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJLCFcRJ5XU0",
        "outputId": "cf3aa86b-8814-4487-e7d2-e88c20020482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trips with payment_type = 1: 4780568\n"
          ]
        }
      ],
      "source": [
        "# Example: Read back only specific partitions (fast due to partition pruning)\n",
        "# When you filter on partition columns, Spark skips reading other partitions\n",
        "\n",
        "# Read all data for payment_type = 1 only\n",
        "df_payment_1 = spark.read.parquet('/content/taxi_data_partitioned_by_payment') \\\n",
        "    .filter('payment_type = 1')\n",
        "\n",
        "print(f\"Trips with payment_type = 1: {df_payment_1.count()}\")\n",
        "\n",
        "# This is much faster than reading all payment types and filtering\n",
        "# because Spark only reads the payment_type=1 directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mbp3y2bp5XU1"
      },
      "source": [
        "---\n",
        "\n",
        "## 23. Advanced Topic: Data Quality Checks & Outlier Detection\n",
        "\n",
        "**What are Outliers?**\n",
        "- Data points that significantly deviate from expected values\n",
        "- Can indicate errors, fraud, or anomalies\n",
        "- Important for data quality and accuracy\n",
        "\n",
        "**Common Detection Methods:**\n",
        "- **Statistical (Z-Score)**: Values beyond mean  3*stddev\n",
        "- **IQR Method**: Values outside Q1-1.5*IQR and Q3+1.5*IQR\n",
        "- **Domain-based**: Known business rules (e.g., fare > $1000)\n",
        "- **Isolation Forest**: Machine learning-based anomaly detection\n",
        "\n",
        "**Use Cases:**\n",
        "- Fraud detection\n",
        "- Data quality validation\n",
        "- Sensor anomaly detection\n",
        "- Financial transaction monitoring\n",
        "- Trip validity checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ETn_01f5XU2",
        "outputId": "9f454b83-fff7-4c79-91d6-775d1c5ec4f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Fare: $25.32\n",
            "StdDev: $329.62\n",
            "\n",
            "Outliers (beyond 3 stddev):\n",
            "Low threshold: $-963.53\n",
            "High threshold: $1014.17\n",
            "Number of outliers: 12\n",
            "+------------+-------------+---------------+\n",
            "|total_amount|trip_distance|passenger_count|\n",
            "+------------+-------------+---------------+\n",
            "|     2506.71|       255.33|              1|\n",
            "|   863380.37|          1.6|              1|\n",
            "|      1311.7|       188.88|              1|\n",
            "|     1869.25|        270.2|              1|\n",
            "|    -1832.85|       268.82|              1|\n",
            "|     1832.85|       268.82|              1|\n",
            "|     2235.17|       457.64|              1|\n",
            "|     -987.94|       202.21|              1|\n",
            "|     1183.59|       174.69|              1|\n",
            "|   132555.41|          2.2|              1|\n",
            "+------------+-------------+---------------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ],
      "source": [
        "# Example: Statistical Outlier Detection using Z-Score method\n",
        "from pyspark.sql.functions import mean, stddev, col, abs\n",
        "\n",
        "# Calculate mean and standard deviation for total_amount\n",
        "fare_stats = df_2025_combined.select(\n",
        "    mean('total_amount').alias('mean_fare'),\n",
        "    stddev('total_amount').alias('stddev_fare')\n",
        ").first()\n",
        "\n",
        "mean_fare = fare_stats['mean_fare']\n",
        "stddev_fare = fare_stats['stddev_fare']\n",
        "\n",
        "print(f\"Mean Fare: ${mean_fare:.2f}\")\n",
        "print(f\"StdDev: ${stddev_fare:.2f}\")\n",
        "\n",
        "# Find outliers: beyond 3 standard deviations\n",
        "outlier_threshold_low = mean_fare - 3 * stddev_fare\n",
        "outlier_threshold_high = mean_fare + 3 * stddev_fare\n",
        "\n",
        "outliers = df_2025_combined.filter(\n",
        "    (col('total_amount') < outlier_threshold_low) |\n",
        "    (col('total_amount') > outlier_threshold_high)\n",
        ")\n",
        "\n",
        "print(f\"\\nOutliers (beyond \\u00b13 stddev):\")\n",
        "print(f\"Low threshold: ${outlier_threshold_low:.2f}\")\n",
        "print(f\"High threshold: ${outlier_threshold_high:.2f}\")\n",
        "print(f\"Number of outliers: {outliers.count()}\")\n",
        "\n",
        "outliers.select('total_amount','trip_distance', 'passenger_count').show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "236sH7nU5XU2",
        "outputId": "421b4c4b-35ff-4594-c90a-b72ac695af4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1: 0.99, Q3: 3.14, IQR: 2.15\n",
            "Lower Bound: -2.24, Upper Bound: 6.37\n",
            "\n",
            "Trip Distance Outliers (IQR method): 832272\n",
            "+-------------+------------+---------------+\n",
            "|trip_distance|total_amount|passenger_count|\n",
            "+-------------+------------+---------------+\n",
            "|          7.2|        40.6|              1|\n",
            "|        10.42|       74.28|              1|\n",
            "|        14.84|       75.55|              1|\n",
            "|        31.97|      111.32|              9|\n",
            "|         7.73|        38.8|              2|\n",
            "|         7.23|       44.04|              1|\n",
            "|         10.9|        57.1|              1|\n",
            "|         8.66|       48.89|              2|\n",
            "|         9.17|       80.28|              2|\n",
            "|          7.2|       46.56|              1|\n",
            "+-------------+------------+---------------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ],
      "source": [
        "# Example: IQR Method for Outlier Detection\n",
        "from pyspark.sql.functions import percentile_approx\n",
        "\n",
        "# Calculate Q1, Q3, and IQR for trip_distance\n",
        "quartiles = df_2025_combined.select(\n",
        "    percentile_approx('trip_distance', 0.25).alias('Q1'),\n",
        "    percentile_approx('trip_distance', 0.75).alias('Q3')\n",
        ").first()\n",
        "\n",
        "Q1 = quartiles['Q1']\n",
        "Q3 = quartiles['Q3']\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "print(f\"Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}\")\n",
        "print(f\"Lower Bound: {lower_bound:.2f}, Upper Bound: {upper_bound:.2f}\")\n",
        "\n",
        "# Find distance outliers\n",
        "distance_outliers = df_2025_combined.filter(\n",
        "    (col('trip_distance') < lower_bound) |\n",
        "    (col('trip_distance') > upper_bound)\n",
        ")\n",
        "\n",
        "print(f\"\\nTrip Distance Outliers (IQR method): {distance_outliers.count()}\")\n",
        "distance_outliers.select('trip_distance', 'total_amount', 'passenger_count').show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcMmOkXk5XU2"
      },
      "source": [
        "### User Defined Functions Challenge\n",
        "\n",
        "**Challenge: Create a Fare Efficiency Score UDF**\n",
        "\n",
        "Calculate a fare efficiency score: (total_fare / trip_distance) per mile.\n",
        "\n",
        "**Steps:**\n",
        "1. Define a Python function `calculate_fare_efficiency()` that returns fare per mile\n",
        "2. Handle cases where distance is 0 or null (return 0 or -1)\n",
        "3. Register it as a FloatType UDF\n",
        "4. Apply it to create a new column `fare_per_mile`\n",
        "5. Find trips with highest and lowest fare efficiency\n",
        "6. Filter for trips with efficiency > $10 per mile (potentially overcharged trips)\n",
        "\n",
        "**Bonus:** Create another UDF to flag suspicious trips (efficiency > mean + 2*stddev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsCqm5R35XU3"
      },
      "source": [
        "---\n",
        "\n",
        "## 24. Advanced Topic: User Defined Functions (UDFs)\n",
        "\n",
        "**What are UDFs?**\n",
        "- Custom Python functions applied to DataFrames\n",
        "- Extend PySpark's built-in functionality\n",
        "- Allow complex business logic in transformations\n",
        "- Can operate on single rows or aggregated data\n",
        "\n",
        "**Types of UDFs:**\n",
        "1. **Standard UDF**: Takes Python function, converts to PySpark UDF\n",
        "2. **Pandas UDF**: Vectorized, uses Apache Arrow, faster performance\n",
        "3. **SQL UDF**: Registered as SQL functions for use in SQL queries\n",
        "\n",
        "**Performance Considerations:**\n",
        "- UDFs are slower than built-in functions (serialization overhead)\n",
        "- Pandas UDFs are faster for vectorized operations\n",
        "- Always try to use built-in functions first\n",
        "\n",
        "**Use Cases:**\n",
        "- Complex business logic transformations\n",
        "- Conditional categorization\n",
        "- String manipulations\n",
        "- Custom validation and categorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxcn0boN5XU3",
        "outputId": "8ae42ce7-12b4-4186-b7a2-9631935aeb19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----------------+\n",
            "|trip_distance|distance_category|\n",
            "+-------------+-----------------+\n",
            "|          1.6|            Short|\n",
            "|          0.5|            Short|\n",
            "|          0.6|            Short|\n",
            "|         0.52|            Short|\n",
            "|         0.66|            Short|\n",
            "|         2.63|           Medium|\n",
            "|          0.4|            Short|\n",
            "|          1.6|            Short|\n",
            "|          2.8|           Medium|\n",
            "|         1.71|            Short|\n",
            "+-------------+-----------------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ],
      "source": [
        "# Example: UDF to categorize trip distance\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Define Python function\n",
        "def categorize_distance(distance):\n",
        "    \"\"\"Categorize trip distance into Short, Medium, or Long\"\"\"\n",
        "    if distance is None:\n",
        "        return 'Unknown'\n",
        "    elif distance < 2:\n",
        "        return 'Short'\n",
        "    elif distance < 10:\n",
        "        return 'Medium'\n",
        "    else:\n",
        "        return 'Long'\n",
        "\n",
        "# Register as UDF with return type StringType\n",
        "distance_category_udf = udf(categorize_distance, StringType())\n",
        "\n",
        "# Apply UDF to create new column\n",
        "df_with_distance_category = df_2025_combined.withColumn(\n",
        "    'distance_category',\n",
        "    distance_category_udf(df_2025_combined['trip_distance'])\n",
        ")\n",
        "\n",
        "# Display results\n",
        "df_with_distance_category.select('trip_distance', 'distance_category').show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKf_1XmA5XU3"
      },
      "source": [
        "### User Defined Functions Challenge\n",
        "\n",
        "**Challenge: Create a Fare Efficiency Score UDF**\n",
        "\n",
        "Calculate a fare efficiency score: (total_fare / trip_distance) per mile.\n",
        "\n",
        "**Steps:**\n",
        "1. Define a Python function `calculate_fare_efficiency()` that returns fare per mile\n",
        "2. Handle cases where distance is 0 or null (return 0 or -1)\n",
        "3. Register it as a FloatType UDF\n",
        "4. Apply it to create a new column `fare_per_mile`\n",
        "5. Find trips with highest and lowest fare efficiency\n",
        "6. Filter for trips with efficiency > $10 per mile (potentially overcharged trips)\n",
        "\n",
        "**Bonus:** Create another UDF to flag suspicious trips (efficiency > mean + 2*stddev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GSOpVMC5XU4"
      },
      "source": [
        "---\n",
        "\n",
        "## 25. Advanced Topic: Pivot Tables\n",
        "\n",
        "**What are Pivot Tables?**\n",
        "- Reorganize and summarize data for reporting\n",
        "- Convert rows into columns (transpose data)\n",
        "- Aggregate data across multiple dimensions\n",
        "- Similar to Excel pivot tables\n",
        "\n",
        "**Syntax:**\n",
        "```python\n",
        "df.groupBy('dimension1').pivot('dimension2').agg(aggregation_function)\n",
        "```\n",
        "\n",
        "\n",
        "**Benefits:**\n",
        "\n",
        "- Easy cross-tabulation of data\n",
        "- Quick multi-dimensional summaries\n",
        "\n",
        "- Better readability for certain analyses\n",
        "\n",
        "- Useful for creating reports and dashboards\n",
        "\n",
        "\n",
        "\n",
        "**Performance Note:**\n",
        "\n",
        "- Can be expensive on large datasets\n",
        "\n",
        "- Works best with limited pivot valuesCreate a pivot table showing the maximum trip distance for each combination of payment type and VendorID.\n",
        "\n",
        "- Use filters to reduce data before pivoting\n",
        "\n",
        "\n",
        "\n",
        "### Pivot Tables Challenge\n",
        "**Challenge: Create Multi-Dimensional Pivot Table**\n",
        "**Steps:**\n",
        "1. Use groupBy() with VendorID\n",
        "2. Pivot on payment_type\n",
        "3. Aggregate using max('trip_distance')\n",
        "4. Display the resulting pivot table\n",
        "5. Interpret the results - which vendor-payment combination has the longest trips?\n",
        "\n",
        "\n",
        "**Bonus:** Create another pivot showing min('total_amount') for the same dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW81Dg4o5XU4",
        "outputId": "23d0c5f5-51ab-48ea-e8a9-d608e92f34d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+-------+-------+-----+------+----+\n",
            "|VendorID|        0|      1|      2|    3|     4|   5|\n",
            "+--------+---------+-------+-------+-----+------+----+\n",
            "|       1|     92.4|44730.3|  265.9| 52.8|  71.3| 0.0|\n",
            "|       6|    32.74|   NULL|   NULL| NULL|  NULL|NULL|\n",
            "|       7|     NULL|  39.72|  62.75|11.96|  2.88|NULL|\n",
            "|       2|276423.57|8002.41|2001.95|81.14|268.82|NULL|\n",
            "+--------+---------+-------+-------+-----+------+----+\n",
            "\n",
            "+--------+------+------+------+------+--------+----+\n",
            "|VendorID|     0|     1|     2|     3|       4|   5|\n",
            "+--------+------+------+------+------+--------+----+\n",
            "|       1|   0.0|   0.0|   0.0|   0.0|     0.0| 0.0|\n",
            "|       6|   0.0|  NULL|  NULL|  NULL|    NULL|NULL|\n",
            "|       7|  NULL|   7.2|   5.9|   5.2|   10.85|NULL|\n",
            "|       2|-35.73|-104.9|-960.0|-901.0|-1832.85|NULL|\n",
            "+--------+------+------+------+------+--------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import max, min\n",
        "\n",
        "# 1. Use groupBy() with VendorID, 2. Pivot on payment_type, 3. Aggregate using max('trip_distance')\n",
        "pivot_max_distance = df_2025_combined.groupBy('VendorID') \\\n",
        "    .pivot('payment_type') \\\n",
        "    .agg(max('trip_distance').alias('max_trip_distance'))\n",
        "\n",
        "# 4. Display the resulting pivot table\n",
        "pivot_max_distance.show()\n",
        "\n",
        "# 5. Interpretation: The highest value in each row shows the longest trip for that vendor/payment type.\n",
        "\n",
        "# Bonus: Create another pivot showing min('total_amount') for the same dimensions\n",
        "pivot_min_total = df_2025_combined.groupBy('VendorID') \\\n",
        "    .pivot('payment_type') \\\n",
        "    .agg(min('total_amount').alias('min_total_amount'))\n",
        "\n",
        "pivot_min_total.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf8a0DuZ5XU4"
      },
      "source": [
        "---\n",
        "\n",
        "## 26. Advanced Topic: Window Functions\n",
        "\n",
        "**What are Window Functions?**\n",
        "- Perform calculations across rows in a DataFrame\n",
        "- Useful for running totals, rankings, and time-series analysis\n",
        "- Apply functions to a \"window\" of rows defined by PARTITION BY and ORDER BY\n",
        "- Return a new column without reducing the number of rows\n",
        "\n",
        "**Key Concepts:**\n",
        "- **PARTITION BY**: Divides data into groups (like GROUP BY but keeps all rows)\n",
        "- **ORDER BY**: Specifies the order within each partition\n",
        "- **Aggregation Functions**: sum, avg, min, max, row_number, rank, dense_rank, lag, lead\n",
        "\n",
        "\n",
        "**Use Cases:**\n",
        "- Running totals and cumulative sums\n",
        "\n",
        "- Ranking within groups\n",
        "\n",
        "- Finding differences from previous row (lag/lead)\n",
        "- Moving averages\n",
        "\n",
        "- Sequential numbering within groups\n",
        "\n",
        "\n",
        "### Window Functions Challenge\n",
        "\n",
        "**Challenge: Calculate Moving Average**\n",
        "**Steps:**\n",
        "1. Import the `lag` function from pyspark.sql.functions\n",
        "2. Create a window that partitions by payment_type and orders by pu_datetime\n",
        "3. Add a new column `prev_fare_1` using lag() to get the previous trip's fare\n",
        "4. Create another column for the fare 2 trips back\n",
        "5. Calculate the average of current and previous 2 trips\n",
        "6. Display results for payment_type = 1 with at least 3 fares\n",
        "\n",
        "Calculate a 3-trip moving average of fare amount for each payment type (use LAG and window functions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBE7WNEk5XU5",
        "outputId": "454d8af3-4067-4fb5-af51-8d125abbc245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------------------+------------+------------------+\n",
            "|payment_type|        pu_datetime|total_amount|running_total_fare|\n",
            "+------------+-------------------+------------+------------------+\n",
            "|           0|2025-01-01 00:00:00|        -3.6|              -3.6|\n",
            "|           0|2025-01-01 00:03:52|        4.89|1.2899999999999996|\n",
            "|           0|2025-01-01 00:04:35|       24.07|             25.36|\n",
            "|           0|2025-01-01 00:04:37|       20.46|             45.82|\n",
            "|           0|2025-01-01 00:04:45|       27.79|             73.61|\n",
            "|           0|2025-01-01 00:04:51|       -4.22|             69.39|\n",
            "|           0|2025-01-01 00:05:18|       22.35| 91.74000000000001|\n",
            "|           0|2025-01-01 00:05:43|       34.52|126.26000000000002|\n",
            "|           0|2025-01-01 00:05:44|       88.68|214.94000000000003|\n",
            "|           0|2025-01-01 00:05:49|       -8.48|206.46000000000004|\n",
            "+------------+-------------------+------------+------------------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ],
      "source": [
        "# Example: Calculate running total of fare amount by payment type\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum as spark_sum, row_number\n",
        "\n",
        "# Define window: partition by payment_type, order by pickup time\n",
        "window_spec = Window.partitionBy('payment_type').orderBy('pu_datetime')\\\n",
        "                     .rowsBetween(Window.unboundedPreceding, 0)\n",
        "\n",
        "# Add running total column\n",
        "df_running_total = df_2025_combined.withColumn(\n",
        "    'running_total_fare',\n",
        "    spark_sum('total_amount').over(window_spec)\n",
        ")\n",
        "\n",
        "# Display first few rows with the running total\n",
        "df_running_total.select('payment_type', 'pu_datetime', 'total_amount', 'running_total_fare').show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUE--qKE5XU5",
        "outputId": "c12a7590-0a87-4798-fc84-76bdc9345dfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------------------+------------+-----------+-----------+---------------+\n",
            "|payment_type|        pu_datetime|total_amount|prev_fare_1|prev_fare_2|moving_avg_fare|\n",
            "+------------+-------------------+------------+-----------+-----------+---------------+\n",
            "|           1|2024-12-31 20:54:50|       39.84|       32.3|      17.16|          29.77|\n",
            "|           1|2024-12-31 21:15:22|        23.6|      39.84|       32.3|          31.91|\n",
            "|           1|2024-12-31 21:20:05|        23.3|       23.6|      39.84|          28.91|\n",
            "|           1|2024-12-31 23:25:38|        92.5|       23.3|       23.6|          46.47|\n",
            "|           1|2024-12-31 23:27:13|        18.0|       92.5|       23.3|           44.6|\n",
            "|           1|2024-12-31 23:30:03|       26.62|       18.0|       92.5|          45.71|\n",
            "|           1|2024-12-31 23:37:42|       14.03|      26.62|       18.0|          19.55|\n",
            "|           1|2024-12-31 23:48:48|        15.9|      14.03|      26.62|          18.85|\n",
            "|           1|2024-12-31 23:49:24|       20.52|       15.9|      14.03|          16.82|\n",
            "|           1|2024-12-31 23:51:20|        69.3|      20.52|       15.9|          35.24|\n",
            "+------------+-------------------+------------+-----------+-----------+---------------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ],
      "source": [
        "# Solution: 3-trip moving average of fare amount for each payment type\n",
        "# 1. Import lag\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import lag, col, round as pyspark_round\n",
        "\n",
        "\n",
        "# 2. Create a window partitioned by payment_type and ordered by pu_datetime\n",
        "window_spec = Window.partitionBy('payment_type').orderBy('pu_datetime')\n",
        "\n",
        "# 3. Add previous fare columns using lag\n",
        "df_with_lags = df_2025_combined \\\n",
        "    .withColumn('prev_fare_1', lag('total_amount', 1).over(window_spec)) \\\n",
        "    .withColumn('prev_fare_2', lag('total_amount', 2).over(window_spec))\n",
        "\n",
        "# 4 & 5. Calculate the 3-trip moving average (current + prev 2) / 3\n",
        "df_with_ma = df_with_lags.withColumn(\n",
        "    'moving_avg_fare',\n",
        "    pyspark_round((\n",
        "        col('total_amount') +\n",
        "        col('prev_fare_1') +\n",
        "        col('prev_fare_2')\n",
        "    ) / 3, 2)\n",
        ")\n",
        "\n",
        "# 6. Show results for payment_type = 1 with at least 3 fares (i.e., not null in all lag columns)\n",
        "df_with_ma.filter(\n",
        "    (col('payment_type') == 1) &\n",
        "    col('prev_fare_1').isNotNull() &\n",
        "    col('prev_fare_2').isNotNull()\n",
        ").select(\n",
        "    'payment_type', 'pu_datetime', 'total_amount', 'prev_fare_1', 'prev_fare_2', 'moving_avg_fare'\n",
        ").show(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "md5fjqhc_DbI"
      },
      "execution_count": 126,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}